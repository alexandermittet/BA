{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "########### PATHS ############\n",
    "pretrain_path =     '/Users/alexandermittet/Library/CloudStorage/SeaDrive-almi(seafile.erda.dk) (14.03.2024 13.32)/My Libraries/BA_data/frames'\n",
    "pretrain_path_imgs = '/Users/alexandermittet/Library/CloudStorage/SeaDrive-almi(seafile.erda.dk) (14.03.2024 13.32)/My Libraries/BA_data/frames/class_0'\n",
    "val_path =          \"/Users/alexandermittet/Library/CloudStorage/SeaDrive-almi(seafile.erda.dk) (14.03.2024 13.32)/My Libraries/BA_data/validation_frames\"\n",
    "val_path_imgs =     \"/Users/alexandermittet/Library/CloudStorage/SeaDrive-almi(seafile.erda.dk) (14.03.2024 13.32)/My Libraries/BA_data/validation_frames/class_0\"\n",
    "fine_tune_path =    '/Users/alexandermittet/Library/CloudStorage/SeaDrive-almi(seafile.erda.dk) (14.03.2024 13.32)/My Libraries/BA_data/img'\n",
    "\n",
    "########### HYPER PARAMETERS ############\n",
    "run_name =          \"mask_ratio=0.5 (ned fra 0.75), derefter tsne\"\n",
    "load_prev_model =   True\n",
    "load_prev_model_path = \"/Users/alexandermittet/Library/CloudStorage/SeaDrive-almi(seafile.erda.dk) (14.03.2024 13.32)/My Libraries/BA_data/models/mae_checkpoint_epoch-60_7198_mask-0.5.pth\"\n",
    "\n",
    "\n",
    "### Pretraining\n",
    "training =          False\n",
    "num_epochs =        100\n",
    "dim =               224\n",
    "batch_size =        32\n",
    "mask_ratio =        0.50\n",
    "\n",
    "### Fine-tuning\n",
    "fine_tuning =       False\n",
    "eval_plot_name =    f\"FiTu_PreTrain_MAE_epochs-{num_epochs}_mask-{mask_ratio}\"\n",
    "num_fine_epochs =   5 #burde jo faktisk nu vÃ¦re 500 med early stopping, \n",
    "                    #da vi fÃ¸r havde en pretrained model som kun skulle fine-tunes i 5 epochs\n",
    "\n",
    "### T-sne feature plotting\n",
    "tsne_plotting =     False\n",
    "tsne_target =       \"training images\"\n",
    "plot_single_image = False\n",
    "plot_16_images =    True\n",
    "\n",
    "### Misc\n",
    "get_num_params =    False\n",
    "break_after_num_steps = -1\n",
    "use_class4 =        False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pytorch-multilabel-balanced-sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_multilabel_balanced_sampler'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 57\u001b[0m\n\u001b[1;32m     48\u001b[0m dataset \u001b[38;5;241m=\u001b[39m ImageFolder(\n\u001b[1;32m     49\u001b[0m         pretrain_path,\n\u001b[1;32m     50\u001b[0m         transform\u001b[38;5;241m=\u001b[39mtransform,\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     53\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m ImageFolder(\n\u001b[1;32m     54\u001b[0m         val_path,\n\u001b[1;32m     55\u001b[0m         transform\u001b[38;5;241m=\u001b[39mtransform,\n\u001b[1;32m     56\u001b[0m     ) \n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_multilabel_balanced_sampler\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msamplers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClassCycleSampler\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Get the class labels (class indices)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m class_indices \u001b[38;5;241m=\u001b[39m [target \u001b[38;5;28;01mfor\u001b[39;00m _, target \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39msamples]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_multilabel_balanced_sampler'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "#import torch.optim as optim #bruger prodigy\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from prodigyopt import Prodigy\n",
    "import lightning as L\n",
    "\n",
    "from einops import repeat, rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from timm.models.layers import trunc_normal_\n",
    "from timm.models.vision_transformer import Block\n",
    "from tqdm import tqdm\n",
    "\n",
    "# class CustomFramesDataset(Dataset):\n",
    "#     def __init__(self, root_dir, transform=None):\n",
    "#         self.root_dir = root_dir\n",
    "#         self.transform = transform\n",
    "#         self.images = []\n",
    "#         for class_dir in os.listdir(root_dir):\n",
    "#             class_path = os.path.join(root_dir, class_dir)\n",
    "#             if os.path.isdir(class_path):\n",
    "#                 for img_name in os.listdir(class_path):\n",
    "#                     img_path = os.path.join(class_path, img_name)\n",
    "#                     image = Image.open(img_path).convert(\"RGB\")\n",
    "#                     if self.transform:\n",
    "#                         image = self.transform(image)\n",
    "#                     self.images.append(image)\n",
    "#         print(f\"Loaded {len(self.images)} images.\") # Debugging line\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.images)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.images[idx]\n",
    "\n",
    "# Define the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((dim, dim)), # Resize the image to #32x32\n",
    "    transforms.ToTensor(), # Convert the image to a PyTorch tensor\n",
    "])\n",
    "\n",
    "#dataset = CustomFramesDataset(root_dir=frames_path, transform=transform)\n",
    "dataset = ImageFolder(\n",
    "        pretrain_path,\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "val_dataset = ImageFolder(\n",
    "        val_path,\n",
    "        transform=transform,\n",
    "    ) \n",
    "from pytorch_multilabel_balanced_sampler.samplers import ClassCycleSampler\n",
    "# Get the class labels (class indices)\n",
    "class_indices = [target for _, target in dataset.samples]\n",
    "\n",
    "# Determine the number of classes\n",
    "num_classes = len(dataset.classes)\n",
    "\n",
    "# Convert class indices to one-hot encoded labels\n",
    "my_labels = torch.eye(num_classes)[class_indices]\n",
    "\n",
    "print(labels)\n",
    "\n",
    "cycle_sampler = ClassCycleSampler(labels=my_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model architecture + masking fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ViT Model + functions\n",
    "class MAE_Encoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_size=dim,\n",
    "                 patch_size=16,\n",
    "                 emb_dim=192,\n",
    "                 num_layer=12,\n",
    "                 num_head=3,\n",
    "                 mask_ratio=mask_ratio,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.cls_token = torch.nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
    "        self.pos_embedding = torch.nn.Parameter(torch.zeros((image_size // patch_size) ** 2, 1, emb_dim))\n",
    "        self.shuffle = PatchShuffle(mask_ratio)\n",
    "\n",
    "        self.patchify = torch.nn.Conv2d(3, emb_dim, patch_size, patch_size)\n",
    "\n",
    "        self.transformer = torch.nn.Sequential(*[Block(emb_dim, num_head) for _ in range(num_layer)])\n",
    "\n",
    "        self.layer_norm = torch.nn.LayerNorm(emb_dim)\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        trunc_normal_(self.pos_embedding, std=.02)\n",
    "\n",
    "    def forward(self, img):\n",
    "        patches = self.patchify(img)\n",
    "        patches = rearrange(patches, 'b c h w -> (h w) b c')\n",
    "        \n",
    "        # Calculate the number of patches\n",
    "        num_patches = patches.shape[0]\n",
    "        \n",
    "\n",
    "        \n",
    "        patches = patches + self.pos_embedding\n",
    "\n",
    "        patches, forward_indexes, backward_indexes = self.shuffle(patches)\n",
    "\n",
    "        patches = torch.cat([self.cls_token.expand(-1, patches.shape[1], -1), patches], dim=0)\n",
    "        patches = rearrange(patches, 't b c -> b t c')\n",
    "        features = self.layer_norm(self.transformer(patches))\n",
    "        features = rearrange(features, 'b t c -> t b c')\n",
    "\n",
    "        return features, backward_indexes\n",
    "\n",
    "class MAE_Decoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_size=dim,\n",
    "                 patch_size=16,\n",
    "                 emb_dim=192,\n",
    "                 num_layer=4,\n",
    "                 num_head=3,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.mask_token = torch.nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
    "        self.pos_embedding = torch.nn.Parameter(torch.zeros((image_size // patch_size) ** 2 + 1, 1, emb_dim))\n",
    "\n",
    "        self.transformer = torch.nn.Sequential(*[Block(emb_dim, num_head) for _ in range(num_layer)])\n",
    "\n",
    "        self.head = torch.nn.Linear(emb_dim, 3 * patch_size ** 2)\n",
    "        self.patch2img = Rearrange('(h w) b (c p1 p2) -> b c (h p1) (w p2)', p1=patch_size, p2=patch_size, h=image_size//patch_size)\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        trunc_normal_(self.mask_token, std=.02)\n",
    "        trunc_normal_(self.pos_embedding, std=.02)\n",
    "\n",
    "    def forward(self, features, backward_indexes):\n",
    "        T = features.shape[0]\n",
    "        backward_indexes = torch.cat([torch.zeros(1, backward_indexes.shape[1]).to(backward_indexes), backward_indexes + 1], dim=0)\n",
    "        features = torch.cat([features, self.mask_token.expand(backward_indexes.shape[0] - features.shape[0], features.shape[1], -1)], dim=0)\n",
    "        features = take_indexes(features, backward_indexes)\n",
    "        features = features + self.pos_embedding\n",
    "\n",
    "        features = rearrange(features, 't b c -> b t c')\n",
    "        features = self.transformer(features)\n",
    "        features = rearrange(features, 'b t c -> t b c')\n",
    "        features = features[1:] # remove global feature\n",
    "\n",
    "        patches = self.head(features)\n",
    "        mask = torch.zeros_like(patches)\n",
    "        mask[T-1:] = 1\n",
    "        mask = take_indexes(mask, backward_indexes[1:] - 1)\n",
    "        img = self.patch2img(patches)\n",
    "        mask = self.patch2img(mask)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "class MAE_ViT(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_size=dim,\n",
    "                 patch_size=16,\n",
    "                 emb_dim=192,\n",
    "                 encoder_layer=12,\n",
    "                 encoder_head=3,\n",
    "                 decoder_layer=4,\n",
    "                 decoder_head=3,\n",
    "                 mask_ratio=mask_ratio,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = MAE_Encoder(image_size, patch_size, emb_dim, encoder_layer, encoder_head, mask_ratio)\n",
    "        self.decoder = MAE_Decoder(image_size, patch_size, emb_dim, decoder_layer, decoder_head)\n",
    "\n",
    "    def forward(self, img):\n",
    "        features, backward_indexes = self.encoder(img)\n",
    "        predicted_img, mask = self.decoder(features,  backward_indexes)\n",
    "        return predicted_img, mask\n",
    "\n",
    "class ViT_Classifier(torch.nn.Module):\n",
    "    def __init__(self, encoder : MAE_Encoder, num_classes=4) -> None:\n",
    "        super().__init__()\n",
    "        self.cls_token = encoder.cls_token\n",
    "        self.pos_embedding = encoder.pos_embedding\n",
    "        self.patchify = encoder.patchify\n",
    "        self.transformer = encoder.transformer\n",
    "        self.layer_norm = encoder.layer_norm\n",
    "        self.head = torch.nn.Linear(self.pos_embedding.shape[-1], num_classes)\n",
    "\n",
    "    def forward(self, img):\n",
    "        patches = self.patchify(img)\n",
    "        patches = rearrange(patches, 'b c h w -> (h w) b c')\n",
    "        patches = patches + self.pos_embedding\n",
    "        patches = torch.cat([self.cls_token.expand(-1, patches.shape[1], -1), patches], dim=0)\n",
    "        patches = rearrange(patches, 't b c -> b t c')\n",
    "        features = self.layer_norm(self.transformer(patches))\n",
    "        features = rearrange(features, 'b t c -> t b c')\n",
    "        logits = self.head(features[0])\n",
    "        return logits\n",
    "    \n",
    "\n",
    "class PatchShuffle(torch.nn.Module):\n",
    "    def __init__(self, ratio) -> None:\n",
    "        super().__init__()\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def forward(self, patches : torch.Tensor):\n",
    "        T, B, C = patches.shape\n",
    "        remain_T = int(T * (1 - self.ratio))\n",
    "\n",
    "        indexes = [random_indexes(T) for _ in range(B)]\n",
    "        forward_indexes = torch.as_tensor(np.stack([i[0] for i in indexes], axis=-1), dtype=torch.long).to(patches.device)\n",
    "        backward_indexes = torch.as_tensor(np.stack([i[1] for i in indexes], axis=-1), dtype=torch.long).to(patches.device)\n",
    "\n",
    "        patches = take_indexes(patches, forward_indexes)\n",
    "        patches = patches[:remain_T]\n",
    "\n",
    "        return patches, forward_indexes, backward_indexes\n",
    "    \n",
    "def random_indexes(size : int):\n",
    "    forward_indexes = np.arange(size)\n",
    "    np.random.shuffle(forward_indexes)\n",
    "    backward_indexes = np.argsort(forward_indexes)\n",
    "    return forward_indexes, backward_indexes\n",
    "\n",
    "def take_indexes(sequences, indexes):\n",
    "    return torch.gather(sequences, 0, repeat(indexes, 't b -> t b c', c=sequences.shape[-1]))\n",
    "\n",
    "def mask_image(image, mask_size):\n",
    "    mask = torch.ones_like(image)\n",
    "    mask[:, :mask_size, :mask_size] = 0\n",
    "    return image * mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model if it exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint not found. Init model from scratch.\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'optimizer' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, optimizer, loss\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_prev_model:\n\u001b[0;32m---> 27\u001b[0m     model, optimizer, loss \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m     model \u001b[38;5;241m=\u001b[39m MAE_ViT()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     model \u001b[38;5;241m=\u001b[39m MAE_ViT()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel checkpoint not found. Init model from scratch.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, \u001b[43moptimizer\u001b[49m, loss\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'optimizer' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "def load_model():\n",
    "    import torch\n",
    "    import os\n",
    "\n",
    "    if os.path.exists(load_prev_model_path):\n",
    "        # Load the checkpoint\n",
    "        checkpoint = torch.load(load_prev_model_path)\n",
    "        \n",
    "        # Load the model state dict\n",
    "        model = MAE_ViT().to(device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # Load the optimizer state dict\n",
    "        optimizer = Prodigy(model.parameters())\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "        # Load the loss\n",
    "        loss = checkpoint['loss']\n",
    "        \n",
    "        print(f\"Model loaded with loss: {loss}\")\n",
    "    else:\n",
    "        model = MAE_ViT().to(device)\n",
    "        print(\"Model checkpoint not found. Init model from scratch.\")\n",
    "    return model, optimizer, loss\n",
    "\n",
    "if load_prev_model:\n",
    "    model, optimizer, loss = load_model()\n",
    "else:\n",
    "    model = MAE_ViT().to(device)\n",
    "    optimizer = Prodigy(model.parameters())\n",
    "    #loss er custom mse loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Num_params in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_num_params:\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training:    #Tensorboard writer\n",
    "    writer = SummaryWriter(os.path.join(\"logs\", \"frames\", \"mae-pretrain\"))\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, sampler=cycle_sampler)\n",
    "    criterion = torch.nn.MSELoss() #pas\n",
    "    \n",
    "\n",
    "    step_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        pbar = tqdm(iter(dataloader))\n",
    "        for img, label in pbar:\n",
    "            step_count += 1\n",
    "            img = img.to(device)\n",
    "            predicted_img, mask = model(img)\n",
    "            loss = (\n",
    "                    torch.mean((predicted_img - img) ** 2 * mask) / mask_ratio\n",
    "                )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            pbar.set_postfix({'step loss': loss.item()}, refresh=False)\n",
    "            writer.add_scalar(\"mae_step_loss\", loss.item(), global_step=step_count)\n",
    "\n",
    "            if break_after_num_steps > 0: #set to -1 to disable early breaking\n",
    "                if step_count >= break_after_num_steps: #Save and quit\n",
    "                    torch.save({\n",
    "                                'model_state_dict': model.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                'loss': loss.item(),\n",
    "                                }, './models/mae_checkpoint_break.pth')\n",
    "                    break\n",
    "        \n",
    "        #For every epoch:\n",
    "        avg_loss = sum(losses) / len(losses)\n",
    "        writer.add_scalar(\"mae_epoch_loss\", avg_loss, global_step=step_count)\n",
    "        print(f\"In epoch {epoch}, average training loss is {avg_loss}.\")\n",
    "\n",
    "        \"\"\" visualize the first 16 predicted images on val dataset\"\"\"\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_img = torch.stack([val_dataset[i][0] for i in range(16)])\n",
    "            val_img = val_img.to(device)\n",
    "            predicted_val_img, mask = model(val_img)\n",
    "            predicted_val_img = predicted_val_img * mask + val_img * (1 - mask)\n",
    "            img = torch.cat(\n",
    "                [val_img * (1 - mask), predicted_val_img, val_img], dim=0\n",
    "            )\n",
    "            img = rearrange(\n",
    "                img, \"(v h1 w1) c h w -> c (h1 h) (w1 v w)\", w1=2, v=3\n",
    "            )\n",
    "            writer.add_image(\"mae_image\", (img + 1) / 2, global_step=epoch)\n",
    "\n",
    "        \"\"\" save model \"\"\"\n",
    "        torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss.item(),\n",
    "                    }, f'/Users/alexandermittet/Library/CloudStorage/SeaDrive-almi(seafile.erda.dk) (14.03.2024 13.32)/My Libraries/BA_data/models/mae_checkpoint_epoch-{epoch}_{step_count}_mask-{mask_ratio}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval 16 imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m val_img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([val_dataset[i][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m16\u001b[39m)])\n\u001b[1;32m      3\u001b[0m val_img \u001b[38;5;241m=\u001b[39m val_img\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m predicted_val_img, mask \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m(val_img)\n\u001b[1;32m      5\u001b[0m predicted_val_img \u001b[38;5;241m=\u001b[39m predicted_val_img \u001b[38;5;241m*\u001b[39m mask \u001b[38;5;241m+\u001b[39m val_img \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m mask)\n\u001b[1;32m      6\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([val_img \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m mask), predicted_val_img, val_img], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "if plot_16_images:\n",
    "    val_img = torch.stack([val_dataset[i][0] for i in range(16)])\n",
    "    val_img = val_img.to(device)\n",
    "    predicted_val_img, mask = model(val_img)\n",
    "    predicted_val_img = predicted_val_img * mask + val_img * (1 - mask)\n",
    "    img = torch.cat([val_img * (1 - mask), predicted_val_img, val_img], dim=0)\n",
    "    img = rearrange(img, \"(v h1 w1) c h w -> (h1 h) (w1 v w) c\", w1=2, v=3)\n",
    "    img_np = img.cpu().detach().numpy()\n",
    "    min_val, max_val = np.min(img_np), np.max(img_np)\n",
    "    range_val = max_val - min_val\n",
    "\n",
    "    # Normalize the data\n",
    "    normalized_img_np = (img_np - min_val) / range_val\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(img_np)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f\"plots/mae_image_16x.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-SNE current results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DO the work one time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_embeddings(input_path):\n",
    "    from torchvision import transforms\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "\n",
    "    import torch\n",
    "\n",
    "    #model, optimizer, loss = load_model() # we do this in another cell\n",
    "    # Extract the encoder\n",
    "    encoder = model.encoder\n",
    "    encoder.eval() # Set the model to evaluation mode\n",
    "\n",
    "    import os\n",
    "\n",
    "    # Specify the directory containing your images\n",
    "    image_dir = input_path\n",
    "    all_files = os.listdir(image_dir)\n",
    "    # Filter out image files (assuming .jpg and .png extensions)\n",
    "    image_files = [file for file in all_files if file.endswith(('.jpg', '.png'))]\n",
    "\n",
    "    # Construct the full paths to the image files\n",
    "    image_paths = [os.path.join(image_dir, file) for file in image_files]\n",
    "    # Load and preprocess images\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)), # Adjust size as needed\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "\n",
    "    from tqdm import tqdm # Import tqdm\n",
    "\n",
    "    # Extract features\n",
    "    features = []\n",
    "    for path in tqdm(image_paths, desc=\"Processing images\"): # Wrap image_paths with tqdm\n",
    "        img = Image.open(path)\n",
    "        img = transform(img).unsqueeze(0) # Add batch dimension\n",
    "        with torch.no_grad():\n",
    "            _, feature = encoder(img)\n",
    "        features.append(feature.squeeze().numpy())\n",
    "\n",
    "    # Stack features into a 2D array\n",
    "    features_array = np.vstack(features)\n",
    "\n",
    "    from sklearn.manifold import TSNE\n",
    "\n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    embedding = tsne.fit_transform(features_array)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    return embedding\n",
    "\n",
    "if tsne_plotting:\n",
    "    if tsne_target == \"training images\":\n",
    "        embedding = tsne_embeddings(pretrain_path_imgs)\n",
    "    elif tsne_target == \"validation images\":\n",
    "        embedding = tsne_embeddings(val_path_imgs)\n",
    "    else:\n",
    "        print(\"Invalid target for t-SNE plotting. Please choose either 'training images' or 'validation images'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just tsne plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:# Plot the 2D embedding\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(embedding[:, 0], embedding[:, 1])\n",
    "    plt.title('t-SNE visualization of image embeddings')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tsne with images on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne(input_path, which_type):\n",
    "    import os\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "    def getImage(path, zoom=.025):\n",
    "        return OffsetImage(plt.imread(path), zoom=zoom)\n",
    "\n",
    "    # Specify the directory containing the images\n",
    "    image_directory = input_path\n",
    "\n",
    "    # List all files in the directory\n",
    "    files = os.listdir(image_directory)\n",
    "\n",
    "    # Filter out the image files (assuming .jpg and .png extensions)\n",
    "    image_files = [os.path.join(image_directory, f) for f in files if f.endswith(('.jpg', '.png'))]\n",
    "\n",
    "    # Create a figure and axes\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Assuming 'embedding' is defined elsewhere in your code\n",
    "    for x0, y0, path in zip(embedding[:, 0], embedding[:, 1], image_files):\n",
    "        ab = AnnotationBbox(getImage(path), (x0, y0), frameon=False)\n",
    "        ax.add_artist(ab)\n",
    "\n",
    "    # Set the limits of the plot to ensure all images are visible\n",
    "    # You might need to adjust these limits based on the range of your embedding coordinates\n",
    "    ax.set_xlim(embedding[:, 0].min(), embedding[:, 0].max())\n",
    "    ax.set_ylim(embedding[:, 1].min(), embedding[:, 1].max())\n",
    "    from datetime import datetime\n",
    "    now = datetime.now()\n",
    "    timestamp = now.strftime(\"%y%m%d_%H%M\")\n",
    "    \n",
    "    title = f\" T-sne plot of feature embeddings of {which_type} images. Mask Ratio: {mask_ratio}\"\n",
    "\n",
    "    # Set the title of the plot\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Save the figure with the datetime stamp in the filename\n",
    "    plt.savefig(f'./plots/tsne_time-{timestamp}_mask-{mask_ratio}_type-{which_type}.png')\n",
    "    plt.show()\n",
    "\n",
    "if tsne_plotting:\n",
    "    if tsne_target == \"training images\":\n",
    "        plot_tsne(pretrain_path_imgs, 'training')\n",
    "    elif tsne_target == \"validation images\":\n",
    "        plot_tsne(val_path_imgs,'validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune model MES score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchsampler import ImbalancedDatasetSampler\n",
    "\n",
    "\n",
    "if fine_tuning == True:\n",
    "    encoder = model.encoder\n",
    "    classifier = ViT_Classifier(encoder, num_classes=4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = Prodigy(classifier.parameters())\n",
    "\n",
    "    class CustomDataset(Dataset):\n",
    "        def __init__(self, df, transform=None):\n",
    "            self.df = df\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.df)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            img_name = self.df.iloc[idx, 0]\n",
    "            img_path = f'{fine_tune_path}/{img_name}' ##### MAN KAN ÃNDRE PATH HER ift /class_0\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            label = torch.tensor(self.df.iloc[idx, 2], dtype=torch.long)\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "        def get_labels(self):\n",
    "            label = torch.tensor(self.df.iloc[:, 2].tolist(), dtype=torch.long)\n",
    "            return label\n",
    "        \n",
    "    def init_dataloaders():\n",
    "            #DATA LABELS\n",
    "        df = pd.read_csv('img_labels_ALL.csv')\n",
    "\n",
    "        # Remove class 4 (images lablelled as bad examples)?\n",
    "        if use_class4 == False:\n",
    "            df = df[df['score'] != 4.0]\n",
    "        #df.head() #Sanity check\n",
    "\n",
    "        # Only use images that exist in the directory\n",
    "        image_folder = fine_tune_path\n",
    "        image_exists = df['img'].apply(lambda x: os.path.isfile(os.path.join(image_folder, x))) \n",
    "        filtered_df = df[image_exists]\n",
    "        print(f\"Original DataFrame size: {len(df)}, Filtered DataFrame size: {len(filtered_df)}\") #Sanity check\n",
    "        df = filtered_df\n",
    "\n",
    "        # Stratify/balance classes across splits\n",
    "        labels = df['score'].values\n",
    "        train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=labels)\n",
    "        train_labels = train_df['score'].values\n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_labels)\n",
    "\n",
    "        # Create data loaders for training and validation sets\n",
    "        test_data = CustomDataset(test_df, transform)\n",
    "        train_data = CustomDataset(train_df, transform)\n",
    "        val_data = CustomDataset(val_df, transform)\n",
    "\n",
    "        #More workers for GPU/lambda\n",
    "        if torch.cuda.is_available():\n",
    "            num_workers_local = 0\n",
    "            test_loader = DataLoader(test_data, batch_size=batch_size, num_workers=num_workers_local)\n",
    "            train_loader = DataLoader(train_data, batch_size=batch_size, sampler=ImbalancedDatasetSampler(train_data))#, num_workers=num_workers_local)\n",
    "            val_loader = DataLoader(val_data, batch_size=batch_size, num_workers=num_workers_local)\n",
    "        else:\n",
    "            test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "            train_loader = DataLoader(train_data, batch_size=batch_size, sampler=ImbalancedDatasetSampler(train_data))\n",
    "            val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "        return train_loader, val_loader, test_loader, test_data\n",
    "\n",
    "    train_loader, val_loader, test_loader, test_data = init_dataloaders()\n",
    "    writer = SummaryWriter(os.path.join(\"logs\", \"MES\", \"MES_fine_tune\"))\n",
    "\n",
    "    step_count = 0\n",
    "    for epoch in range(num_fine_epochs):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        pbar = tqdm(iter(train_loader))\n",
    "        classifier.train() # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        for images, labels in pbar:\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = classifier(images)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            pbar.set_postfix({'step loss': loss.item()}, refresh=False)\n",
    "            writer.add_scalar('Fine_tune_loss/train', loss.item(), global_step=step_count)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update running loss\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        # Compute average loss over the epoch\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        writer.add_scalar('Fine_tune_loss/train-epoch', epoch_loss, global_step=step_count)\n",
    "        \n",
    "        # Validation loop\n",
    "        classifier.eval() # Set the model to evaluation mode\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = classifier(images)\n",
    "                val_loss = criterion(outputs, labels)\n",
    "                running_val_loss += val_loss.item() * images.size(0)\n",
    "        \n",
    "        # Compute average validation loss over the epoch\n",
    "        val_epoch_loss = running_val_loss / len(val_loader.dataset)\n",
    "        writer.add_scalar('Fine_tune_loss/val-epoch', val_epoch_loss, global_step=step_count)\n",
    "\n",
    "        #save model after each epoch\n",
    "        torch.save({\n",
    "                    'model_state_dict': classifier.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss.item(),\n",
    "                    }, f'/Users/alexandermittet/Library/CloudStorage/SeaDrive-almi(seafile.erda.dk) (14.03.2024 13.32)/My Libraries/BA_data/models/mes_ckpt_epoch-{epoch}_{step_count}_valloss_{val_epoch_loss}.pth')\n",
    "\n",
    "    print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fine_tuning == True:    \n",
    "    #from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    def evaluate_single_sample(i):\n",
    "        # Get the sample from the dataset\n",
    "        sample, true_label = test_data[i]\n",
    "        \n",
    "        # Move the sample to the same device as the model\n",
    "        sample = sample.to(device)\n",
    "        \n",
    "        # Pass the sample through the model\n",
    "        with torch.no_grad():\n",
    "            prediction = model(sample.unsqueeze(0))  # Unsqueeze to add batch dimension\n",
    "        \n",
    "        class_probabilities = prediction[0]\n",
    "        _, predicted_class = torch.max(class_probabilities, dim=1)\n",
    "\n",
    "        # Move the predicted_class back to CPU for further operations\n",
    "        predicted_class = predicted_class.to('cpu')\n",
    "        \n",
    "        # Return the true label and the predicted class\n",
    "        return true_label.item(), predicted_class.item()\n",
    "\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Shared lists to store true labels and predicted classes\n",
    "    true_labels_list = []\n",
    "    predicted_classes_list = []\n",
    "\n",
    "    # Create a thread pool executor\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        # Evaluate all samples in parallel and collect results\n",
    "        results = list(tqdm(executor.map(evaluate_single_sample, range(len(test_data))), total=len(test_data), desc='Evaluating'))\n",
    "\n",
    "    # Collect the true labels and predicted classes from the results\n",
    "    for true_label, predicted_class in results:\n",
    "        true_labels_list.append(true_label)\n",
    "        predicted_classes_list.append(predicted_class)\n",
    "\n",
    "    # Assuming `true_labels` and `predicted_labels` are already defined\n",
    "    f1 = f1_score(true_labels_list, predicted_classes_list, average='weighted')\n",
    "    precision = precision_score(true_labels_list, predicted_classes_list, average='weighted')\n",
    "    recall = recall_score(true_labels_list, predicted_classes_list, average='weighted')\n",
    "    cm = confusion_matrix(true_labels_list, predicted_classes_list)\n",
    "\n",
    "    # Create a figure and a 2x2 grid of subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # Confusion matrix subplot\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axs[0, 0])\n",
    "    axs[0, 0].set_title('Confusion Matrix')\n",
    "    axs[0, 0].set_xlabel('Predicted')\n",
    "    axs[0, 0].set_ylabel('True')\n",
    "\n",
    "    # F1 Score subplot\n",
    "    axs[0, 1].text(0.5, 0.5, f'F1 Score: {f1:.2f}', horizontalalignment='center', verticalalignment='center', fontsize=14)\n",
    "    axs[0, 1].axis('off')\n",
    "\n",
    "    # Precision subplot\n",
    "    axs[1, 0].text(0.5, 0.5, f'Precision: {precision:.2f}', horizontalalignment='center', verticalalignment='center', fontsize=14)\n",
    "    axs[1, 0].axis('off')\n",
    "\n",
    "    # Recall subplot\n",
    "    axs[1, 1].text(0.5, 0.5, f'Recall: {recall:.2f}', horizontalalignment='center', verticalalignment='center', fontsize=14)\n",
    "    axs[1, 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.title('Confusion Matrix of MES-scores')\n",
    "\n",
    "    plt.savefig(f'./plots/eval_plot-{eval_plot_name}.png')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
