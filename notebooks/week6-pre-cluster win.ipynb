{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# wandb.init(project=\"BA-MAE\", entity=\"alexandermittet\")\n",
    "\n",
    "########### PATHS ############\n",
    "pretrain_path =     r'C:\\Users\\alx\\Downloads\\BA_data\\frames'\n",
    "#for t-sne:\n",
    "pretrain_path_imgs = '/Users/alexandermittet/Library/CloudStorage/SeaDrive-almi(seafile.erda.dk) (14.03.2024 13.32)/My Libraries/BA_data/frames/class_0'\n",
    "val_path =          r'C:\\Users\\alx\\Downloads\\BA_data\\val_frames'\n",
    "#for t-sne:\n",
    "val_path_imgs =     \"/Users/alexandermittet/Library/CloudStorage/SeaDrive-almi(seafile.erda.dk) (14.03.2024 13.32)/My Libraries/BA_data/validation_frames/class_0\"\n",
    "fine_tune_path =    'C:\\\\Users\\\\alx\\\\Downloads\\\\img\\\\img'\n",
    "\n",
    "########### HYPER PARAMETERS ############\n",
    "### MODEL SIZE\n",
    "emb_dims =          192#192 #skal ku duvuderes med num_heads som er 4 \n",
    "                    #85M params i TIMM classifiers, så den har vi smags til at 190*4 passer med 12 layers\n",
    "encoder_layers =    12#12 i timm\n",
    "\n",
    "\n",
    "### Global\n",
    "custom_opt =        \"adamW\" #bliver også brugt til fine tuning\n",
    "learn_rate =        0.001\n",
    "batch_size =        64\n",
    "load_prev_model_path        = r\"C:\\Users\\alx\\Downloads\\mdl_ckpt\\mae_m-0.5\\mae_best.pth\"\n",
    "load_prev_model =   True #mAe model load\n",
    "model_used =        \"last\"\n",
    "load_prev_fine_model_path   = f'C:\\\\Users\\\\alx\\\\Downloads\\\\mdl_ckpt\\\\mes_m-0.5\\\\mes_{model_used}.pth' #\"den skal loade den som den selv gemmer\"\n",
    "load_fine_tuned =   True #meS vi bliver nødt til, for at evaluere MES best.pth (og ikke last, som vi stadig ligger i memory)\n",
    "\n",
    "### Pretraining\n",
    "training =          False #Pre-training MAE\n",
    "num_epochs =        33\n",
    "dim =               224\n",
    "mask_ratio =        0.50\n",
    "\n",
    "### Fine-tuning\n",
    "fine_tuning =       False #MES training\n",
    "num_fine_epochs =   20 #overfitter efter færre faktisk, men vi gemmer også best\n",
    "fine_tune_eval =    True\n",
    "\n",
    "\n",
    "#370K betyder 370K training MAE images (og ? i validation fordi det er lidt ligegyldigt, bare ca 20/80 split)\n",
    "eval_plot_name =    f\"MAE-370K_optim-{custom_opt}_max-epoch-{num_epochs}_bs-{batch_size}_mask-{mask_ratio}_embdims-{emb_dims}_fine-e-{num_fine_epochs}_run-x\"\n",
    "\n",
    "\n",
    "### T-sne feature plotting\n",
    "tsne_plotting =     False\n",
    "tsne_target =       \"validation images\"\n",
    "plot_single_image = False\n",
    "plot_16_images =    True\n",
    "\n",
    "### Misc\n",
    "get_num_params =    False\n",
    "break_after_num_steps = -1\n",
    "use_class4 =        False\n",
    "\n",
    "run_name =          f\"opt_{custom_opt}_max-e-{num_epochs}_m-{mask_ratio}_fine-e-{num_fine_epochs}_bs-{batch_size}_emb_dim-{emb_dims}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\alx\\\\Downloads\\\\BA_data\\\\frames'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m\n\u001b[1;32m     22\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     23\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((dim, dim)), \u001b[38;5;66;03m# Resize the image to #32x32\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(), \u001b[38;5;66;03m# Convert the image to a PyTorch tensor\u001b[39;00m\n\u001b[1;32m     25\u001b[0m ])\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#dataset = CustomFramesDataset(root_dir=frames_path, transform=transform)\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrain_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m ImageFolder(\n\u001b[1;32m     34\u001b[0m         val_path,\n\u001b[1;32m     35\u001b[0m         transform\u001b[38;5;241m=\u001b[39mtransform,\n\u001b[1;32m     36\u001b[0m     ) \n",
      "File \u001b[0;32m~/miniconda3/envs/BA/lib/python3.11/site-packages/torchvision/datasets/folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    303\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    308\u001b[0m ):\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[0;32m~/miniconda3/envs/BA/lib/python3.11/site-packages/torchvision/datasets/folder.py:144\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    136\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    142\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 144\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[0;32m~/miniconda3/envs/BA/lib/python3.11/site-packages/torchvision/datasets/folder.py:218\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m    192\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/BA/lib/python3.11/site-packages/torchvision/datasets/folder.py:40\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\alx\\\\Downloads\\\\BA_data\\\\frames'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "#import torch.optim as optim #bruger prodigy\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from prodigyopt import Prodigy\n",
    "from torch.optim import AdamW\n",
    "import lightning as L\n",
    "# import wandb\n",
    "\n",
    "from einops import repeat, rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from timm.models.layers import trunc_normal_\n",
    "from timm.models.vision_transformer import Block\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((dim, dim)), # Resize the image to #32x32\n",
    "    transforms.ToTensor(), # Convert the image to a PyTorch tensor\n",
    "])\n",
    "\n",
    "#dataset = CustomFramesDataset(root_dir=frames_path, transform=transform)\n",
    "dataset = ImageFolder(\n",
    "        pretrain_path,\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "val_dataset = ImageFolder(\n",
    "        val_path,\n",
    "        transform=transform,\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model architecture + masking fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ViT Model + functions\n",
    "class MAE_Encoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_size=dim,\n",
    "                 patch_size=16,\n",
    "                 emb_dim=emb_dims,\n",
    "                 num_layer=encoder_layers,\n",
    "                 num_head=4,\n",
    "                 mask_ratio=mask_ratio,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.cls_token = torch.nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
    "        self.pos_embedding = torch.nn.Parameter(torch.zeros((image_size // patch_size) ** 2, 1, emb_dim))\n",
    "        self.shuffle = PatchShuffle(mask_ratio)\n",
    "\n",
    "        self.patchify = torch.nn.Conv2d(3, emb_dim, patch_size, patch_size)\n",
    "\n",
    "        self.transformer = torch.nn.Sequential(*[Block(emb_dim, num_head) for _ in range(num_layer)])\n",
    "\n",
    "        self.layer_norm = torch.nn.LayerNorm(emb_dim)\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        trunc_normal_(self.pos_embedding, std=.02)\n",
    "\n",
    "    def forward(self, img):\n",
    "        patches = self.patchify(img)\n",
    "        patches = rearrange(patches, 'b c h w -> (h w) b c')\n",
    "        \n",
    "        # Calculate the number of patches\n",
    "        num_patches = patches.shape[0]\n",
    "        \n",
    "\n",
    "        \n",
    "        patches = patches + self.pos_embedding\n",
    "\n",
    "        patches, forward_indexes, backward_indexes = self.shuffle(patches)\n",
    "\n",
    "        patches = torch.cat([self.cls_token.expand(-1, patches.shape[1], -1), patches], dim=0)\n",
    "        patches = rearrange(patches, 't b c -> b t c')\n",
    "        features = self.layer_norm(self.transformer(patches))\n",
    "        features = rearrange(features, 'b t c -> t b c')\n",
    "\n",
    "        return features, backward_indexes\n",
    "\n",
    "class MAE_Decoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_size=dim,\n",
    "                 patch_size=16,\n",
    "                 emb_dim=192,\n",
    "                 num_layer=4,\n",
    "                 num_head=3,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.mask_token = torch.nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
    "        self.pos_embedding = torch.nn.Parameter(torch.zeros((image_size // patch_size) ** 2 + 1, 1, emb_dim))\n",
    "\n",
    "        self.transformer = torch.nn.Sequential(*[Block(emb_dim, num_head) for _ in range(num_layer)])\n",
    "\n",
    "        self.head = torch.nn.Linear(emb_dim, 3 * patch_size ** 2)\n",
    "        self.patch2img = Rearrange('(h w) b (c p1 p2) -> b c (h p1) (w p2)', p1=patch_size, p2=patch_size, h=image_size//patch_size)\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        trunc_normal_(self.mask_token, std=.02)\n",
    "        trunc_normal_(self.pos_embedding, std=.02)\n",
    "\n",
    "    def forward(self, features, backward_indexes):\n",
    "        T = features.shape[0]\n",
    "        backward_indexes = torch.cat([torch.zeros(1, backward_indexes.shape[1]).to(backward_indexes), backward_indexes + 1], dim=0)\n",
    "        features = torch.cat([features, self.mask_token.expand(backward_indexes.shape[0] - features.shape[0], features.shape[1], -1)], dim=0)\n",
    "        features = take_indexes(features, backward_indexes)\n",
    "        features = features + self.pos_embedding\n",
    "\n",
    "        features = rearrange(features, 't b c -> b t c')\n",
    "        features = self.transformer(features)\n",
    "        features = rearrange(features, 'b t c -> t b c')\n",
    "        features = features[1:] # remove global feature\n",
    "\n",
    "        patches = self.head(features)\n",
    "        mask = torch.zeros_like(patches)\n",
    "        mask[T-1:] = 1\n",
    "        mask = take_indexes(mask, backward_indexes[1:] - 1)\n",
    "        img = self.patch2img(patches)\n",
    "        mask = self.patch2img(mask)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "class MAE_ViT(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_size=dim,\n",
    "                 patch_size=16,\n",
    "                 emb_dim=emb_dims,#192,\n",
    "                 encoder_layer=encoder_layers,#12,\n",
    "                 encoder_head=4,\n",
    "                 decoder_layer=4,\n",
    "                 decoder_head=4,\n",
    "                 mask_ratio=mask_ratio,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = MAE_Encoder(image_size, patch_size, emb_dim, encoder_layer, encoder_head, mask_ratio)\n",
    "        self.decoder = MAE_Decoder(image_size, patch_size, emb_dim, decoder_layer, decoder_head)\n",
    "\n",
    "    def forward(self, img):\n",
    "        features, backward_indexes = self.encoder(img)\n",
    "        predicted_img, mask = self.decoder(features,  backward_indexes)\n",
    "        return predicted_img, mask\n",
    "\n",
    "class ViT_Classifier(torch.nn.Module):\n",
    "    def __init__(self, encoder : MAE_Encoder, num_classes=4) -> None:\n",
    "        super().__init__()\n",
    "        self.cls_token = encoder.cls_token\n",
    "        self.pos_embedding = encoder.pos_embedding\n",
    "        self.patchify = encoder.patchify\n",
    "        self.transformer = encoder.transformer\n",
    "        self.layer_norm = encoder.layer_norm\n",
    "        self.head = torch.nn.Linear(self.pos_embedding.shape[-1], num_classes)\n",
    "\n",
    "    def forward(self, img):\n",
    "        patches = self.patchify(img)\n",
    "        patches = rearrange(patches, 'b c h w -> (h w) b c')\n",
    "        patches = patches + self.pos_embedding\n",
    "        patches = torch.cat([self.cls_token.expand(-1, patches.shape[1], -1), patches], dim=0)\n",
    "        patches = rearrange(patches, 't b c -> b t c')\n",
    "        features = self.layer_norm(self.transformer(patches))\n",
    "        features = rearrange(features, 'b t c -> t b c')\n",
    "        logits = self.head(features[0])\n",
    "        return logits\n",
    "    \n",
    "\n",
    "class PatchShuffle(torch.nn.Module):\n",
    "    def __init__(self, ratio) -> None:\n",
    "        super().__init__()\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def forward(self, patches : torch.Tensor):\n",
    "        T, B, C = patches.shape\n",
    "        remain_T = int(T * (1 - self.ratio))\n",
    "\n",
    "        indexes = [random_indexes(T) for _ in range(B)]\n",
    "        forward_indexes = torch.as_tensor(np.stack([i[0] for i in indexes], axis=-1), dtype=torch.long).to(patches.device)\n",
    "        backward_indexes = torch.as_tensor(np.stack([i[1] for i in indexes], axis=-1), dtype=torch.long).to(patches.device)\n",
    "\n",
    "        patches = take_indexes(patches, forward_indexes)\n",
    "        patches = patches[:remain_T]\n",
    "\n",
    "        return patches, forward_indexes, backward_indexes\n",
    "    \n",
    "def random_indexes(size : int):\n",
    "    forward_indexes = np.arange(size)\n",
    "    np.random.shuffle(forward_indexes)\n",
    "    backward_indexes = np.argsort(forward_indexes)\n",
    "    return forward_indexes, backward_indexes\n",
    "\n",
    "def take_indexes(sequences, indexes):\n",
    "    return torch.gather(sequences, 0, repeat(indexes, 't b -> t b c', c=sequences.shape[-1]))\n",
    "\n",
    "def mask_image(image, mask_size):\n",
    "    mask = torch.ones_like(image)\n",
    "    mask[:, :mask_size, :mask_size] = 0\n",
    "    return image * mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load  pre-train model if it exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    import torch\n",
    "    import os\n",
    "\n",
    "    if os.path.exists(load_prev_model_path):\n",
    "        # Load the checkpoint\n",
    "        checkpoint = torch.load(load_prev_model_path)\n",
    "        \n",
    "        # Load the model state dict\n",
    "        model = MAE_ViT().to(device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # Load the optimizer state dict\n",
    "        if custom_opt == \"prodigy\":\n",
    "            optimizer = Prodigy(model.parameters())\n",
    "        elif custom_opt == \"adamW\":\n",
    "            optimizer = AdamW(model.parameters(), lr=learn_rate)\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "        # Load the loss\n",
    "        loss = checkpoint['loss']\n",
    "        \n",
    "        print(f\"Model loaded with loss: {loss}\")\n",
    "    else:\n",
    "        model = MAE_ViT().to(device)\n",
    "        print(\"Model checkpoint not found. Init model from scratch.\")\n",
    "    return model, optimizer, loss\n",
    "\n",
    "if load_prev_model:\n",
    "    model, optimizer, loss = load_model()\n",
    "else:\n",
    "    model = MAE_ViT().to(device)\n",
    "    if custom_opt == \"adamW\":\n",
    "        learning_rate = learn_rate # Example learning rate\n",
    "        optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    elif custom_opt == \"prodigy\":\n",
    "        optimizer = Prodigy(model.parameters())\n",
    "    #loss er custom mse loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Num_params in MAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_num_params or plot_16_images:\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total number of parameters: {total_params:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsampler import ImbalancedDatasetSampler\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, sampler=ImbalancedDatasetSampler(dataset)) #sampler=cycle_sampler\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, sampler=ImbalancedDatasetSampler(val_dataset))\n",
    "   \n",
    "if training:    #Tensorboard writer\n",
    "    \n",
    "    import random\n",
    "    from datetime import datetime\n",
    "    now = datetime.now()\n",
    "    # Format the datetime string to exclude the year\n",
    "    formatted_datetime = now.strftime(\"%m-%d_%H-%M\")\n",
    "    # Use the formatted datetime in the path\n",
    "    writer = SummaryWriter(os.path.join(\"logs\", \"mae\", f\"{formatted_datetime}_{run_name}\"))\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, sampler=ImbalancedDatasetSampler(dataset)) #sampler=cycle_sampler\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, sampler=ImbalancedDatasetSampler(val_dataset))\n",
    "    #criterion = torch.nn.MSELoss() #pas\n",
    "    val_iterator = iter(val_loader)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    step_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        pbar = tqdm(iter(dataloader))\n",
    "        for img, label in pbar:\n",
    "            step_count += 1\n",
    "            img = img.to(device)\n",
    "            predicted_img, mask = model(img)\n",
    "            loss = (\n",
    "                    torch.mean((predicted_img - img) ** 2 * mask) / mask_ratio\n",
    "                )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            pbar.set_postfix({'step loss': loss.item()}, refresh=False)\n",
    "            writer.add_scalar(\"mae/step__train_loss\", loss.item(), global_step=step_count)\n",
    "            # wandb.log({\"Train Loss step\": loss})\n",
    "\n",
    "            if break_after_num_steps > 0: #set to -1 to disable early breaking\n",
    "                if step_count >= break_after_num_steps: #Save and quit\n",
    "                    # torch.save({\n",
    "                    #             'model_state_dict': model.state_dict(),\n",
    "                    #             'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    #             'loss': loss.item(),\n",
    "                    #             }, './models/mae_checkpoint_break.pth')\n",
    "                    break\n",
    "        \n",
    "        #For every epoch:\n",
    "        avg_loss = sum(losses) / len(losses)\n",
    "        writer.add_scalar(\"mae/epoch_train_loss\", avg_loss, global_step=step_count)\n",
    "        print(f\"In epoch {epoch}, average training loss is {avg_loss}.\")\n",
    "        # wandb.log({\"Train Loss epoch avg\": avg_loss})\n",
    "\n",
    "        \"\"\" visualize the first 16 predicted images on val dataset\"\"\"\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_bar = tqdm(iter(val_loader))\n",
    "        with torch.no_grad():\n",
    "            for img, label in val_bar:\n",
    "                img = img.to(device)\n",
    "                predicted_img, mask = model(img)\n",
    "                val_loss = (\n",
    "                        torch.mean((predicted_img - img) ** 2 * mask) / mask_ratio\n",
    "                    )\n",
    "                val_losses.append(val_loss.item())\n",
    "                #break #vi vil gerne køre alt validatio igennem\n",
    "            \n",
    "            avg_val_loss = torch.mean(torch.tensor(val_losses))\n",
    "            writer.add_scalar(\"mae/epoch_val_loss\", avg_val_loss.item(), global_step=step_count)\n",
    "\n",
    "            # #converting for 16x plotting\n",
    "            \n",
    "            # selected_classes = random.sample(range(20), 16) #classes=20, and images to load=16\n",
    "\n",
    "            # # List comprehension to select one image from each class\n",
    "            # selected_images = [next(val_dataset[i][0] for i in range(len(val_dataset)) if val_dataset[i][1] == class_idx) for class_idx in selected_classes]\n",
    "\n",
    "            # # Stack the selected images into a single tensor\n",
    "            # val_img = torch.stack(selected_images)\n",
    "            # ##val_img = torch.stack([val_dataset[i][0] for i in range(16)]) #using random batch from val_loader instead\n",
    "            # predicted_val_img = predicted_val_img * mask + val_img * (1 - mask)\n",
    "            # img = torch.cat(\n",
    "            #     [val_img * (1 - mask), predicted_val_img, val_img], dim=0\n",
    "            # )\n",
    "            # img = rearrange(\n",
    "            #     img, \"(v h1 w1) c h w -> c (h1 h) (w1 v w)\", w1=2, v=3\n",
    "            # )\n",
    "            # writer.add_image(\"mae/16imgs\", (img + 1) / 2, global_step=epoch)\n",
    "            # # transform = transforms.ToPILImage()\n",
    "            # # pil_img = transform(img)\n",
    "            # # wandb_img = wandb.Image(pil_img)\n",
    "            # # wandb.log({\"16 val mae_images\": wandb_img})\n",
    "\n",
    "        \"\"\" save model \"\"\"\n",
    "        # base_path = \"C:\\\\Users\\\\alx\\\\Downloads\\\\mdl_ckptmae_checkpoint_epoch\"\n",
    "        # torch.save({\n",
    "        #             'model_state_dict': model.state_dict(),\n",
    "        #             'optimizer_state_dict': optimizer.state_dict(),\n",
    "        #             'loss': loss.item(),\n",
    "        #             }, os.path.join(base_path, f\"-{epoch}_{step_count}_mask-{mask_ratio}.pth\"))\n",
    "        if val_loss < best_val_loss:\n",
    "            # Update the best loss\n",
    "            best_val_loss = val_loss\n",
    "            \n",
    "            # Construct the filename with the current loss included\n",
    "            filename = f\"mae_best.pth\"\n",
    "        else:\n",
    "            filename = \"mae_last.pth\"\n",
    "        # Construct the full path using os.path.join\n",
    "        base_path = f\"C:\\\\Users\\\\alx\\\\Downloads\\\\mdl_ckpt\\\\mae_m-{mask_ratio}\"\n",
    "        full_path = os.path.join(base_path, filename)\n",
    "        \n",
    "        # Save the model\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss.item(),\n",
    "        }, full_path)\n",
    "            \n",
    "        print(f\"Model saved with avg loss {avg_loss:.4f} at {full_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval 16 imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_16_images:\n",
    "    if training == False:\n",
    "        step_count = 1050\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "    num_samples = 16\n",
    "    indices = random.sample(range(len(val_dataset)), num_samples)\n",
    "    val_samples = [val_dataset[i][0] for i in indices]\n",
    "    val_img = torch.stack(val_samples)\n",
    "    #val_img = torch.stack([val_dataset[i][0] for i in range(21)])\n",
    "    \n",
    "\n",
    "    val_img = val_img.to(device)\n",
    "    predicted_val_img, mask = model(val_img)\n",
    "    predicted_val_img = predicted_val_img * mask + val_img * (1 - mask)\n",
    "    img = torch.cat([val_img * (1 - mask), predicted_val_img, val_img], dim=0)\n",
    "    img = rearrange(img, \"(v h1 w1) c h w -> (h1 h) (w1 v w) c\", w1=2, v=3)\n",
    "    img_np = img.cpu().detach().numpy()\n",
    "    min_val, max_val = np.min(img_np), np.max(img_np)\n",
    "    range_val = max_val - min_val\n",
    "\n",
    "    # Normalize the data\n",
    "    normalized_img_np = (img_np - min_val) / range_val\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.axis('off')\n",
    "    plt.title(f'MAE: \\n 16 first val imgs. MAE pretrained for: {num_epochs}eps, masking: {mask_ratio} \\n MAE model has {total_params:,.0f} num params')\n",
    "    plt.imshow(normalized_img_np)\n",
    "    plt.savefig(f\"plots/mae_image_16x_mae-step-{step_count}_m-{mask_ratio}_params-{total_params:,.0f}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-SNE current results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DO the work one time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embedding\n\u001b[0;32m---> 54\u001b[0m \u001b[43mtsne_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_path_imgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tsne_plotting:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tsne_target \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining images\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m, in \u001b[0;36mtsne_embeddings\u001b[0;34m(input_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#model, optimizer, loss = load_model() # we do this in another cell\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Extract the encoder\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mencoder\n\u001b[1;32m     11\u001b[0m encoder\u001b[38;5;241m.\u001b[39meval() \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def tsne_embeddings(input_path):\n",
    "    from torchvision import transforms\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "\n",
    "    import torch\n",
    "\n",
    "    #model, optimizer, loss = load_model() # we do this in another cell\n",
    "    # Extract the encoder\n",
    "    encoder = model.encoder\n",
    "    encoder.eval() # Set the model to evaluation mode\n",
    "\n",
    "    import os\n",
    "\n",
    "    # Specify the directory containing your images\n",
    "    image_dir = input_path\n",
    "    all_files = os.listdir(image_dir)\n",
    "    # Filter out image files (assuming .jpg and .png extensions)\n",
    "    image_files = [file for file in all_files if file.endswith(('.jpg', '.png'))]\n",
    "\n",
    "    # Construct the full paths to the image files\n",
    "    image_paths = [os.path.join(image_dir, file) for file in image_files]\n",
    "    # Load and preprocess images\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)), # Adjust size as needed\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "\n",
    "    from tqdm import tqdm # Import tqdm\n",
    "\n",
    "    # Extract features\n",
    "    features = []\n",
    "    for path in tqdm(image_paths, desc=\"Processing images\"): # Wrap image_paths with tqdm\n",
    "        img = Image.open(path)\n",
    "        img = transform(img).unsqueeze(0) # Add batch dimension\n",
    "        with torch.no_grad():\n",
    "            _, feature = encoder(img)\n",
    "            print(feature.shape)\n",
    "            \n",
    "        features.append(feature.squeeze().numpy())\n",
    "\n",
    "    # Stack features into a 2D array\n",
    "    features_array = np.vstack(features)\n",
    "\n",
    "    from sklearn.manifold import TSNE\n",
    "\n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    embedding = tsne.fit_transform(features_array)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    return embedding\n",
    "\n",
    "if tsne_plotting:\n",
    "    if tsne_target == \"training images\":\n",
    "        embedding = tsne_embeddings(pretrain_path_imgs)\n",
    "    elif tsne_target == \"validation images\":\n",
    "        embedding = tsne_embeddings(val_path_imgs)\n",
    "    else:\n",
    "        print(\"Invalid target for t-SNE plotting. Please choose either 'training images' or 'validation images'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just tsne plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:# Plot the 2D embedding\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(embedding[:, 0], embedding[:, 1])\n",
    "    plt.title('t-SNE visualization of image embeddings')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tsne with images on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne(input_path, which_type):\n",
    "    import os\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "    def getImage(path, zoom=.025):\n",
    "        return OffsetImage(plt.imread(path), zoom=zoom)\n",
    "\n",
    "    # Specify the directory containing the images\n",
    "    image_directory = input_path\n",
    "\n",
    "    # List all files in the directory\n",
    "    files = os.listdir(image_directory)\n",
    "\n",
    "    # Filter out the image files (assuming .jpg and .png extensions)\n",
    "    image_files = [os.path.join(image_directory, f) for f in files if f.endswith(('.jpg', '.png'))]\n",
    "\n",
    "    # Create a figure and axes\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Assuming 'embedding' is defined elsewhere in your code\n",
    "    for x0, y0, path in zip(embedding[:, 0], embedding[:, 1], image_files):\n",
    "        ab = AnnotationBbox(getImage(path), (x0, y0), frameon=False)\n",
    "        ax.add_artist(ab)\n",
    "\n",
    "    # Set the limits of the plot to ensure all images are visible\n",
    "    # You might need to adjust these limits based on the range of your embedding coordinates\n",
    "    ax.set_xlim(embedding[:, 0].min(), embedding[:, 0].max())\n",
    "    ax.set_ylim(embedding[:, 1].min(), embedding[:, 1].max())\n",
    "    from datetime import datetime\n",
    "    now = datetime.now()\n",
    "    timestamp = now.strftime(\"%y%m%d_%H%M\")\n",
    "    \n",
    "    title = f\" T-sne plot of feature embeddings of {which_type} images. Mask Ratio: {mask_ratio}\"\n",
    "\n",
    "    # Set the title of the plot\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Save the figure with the datetime stamp in the filename\n",
    "    plt.savefig(f'./plots/tsne_time-{timestamp}_mask-{mask_ratio}_type-{which_type}.png')\n",
    "    plt.show()\n",
    "\n",
    "if tsne_plotting:\n",
    "    if tsne_target == \"training images\":\n",
    "        plot_tsne(pretrain_path_imgs, 'training')\n",
    "    elif tsne_target == \"validation images\":\n",
    "        plot_tsne(val_path_imgs,'validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune model MES score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchsampler import ImbalancedDatasetSampler\n",
    "encoder = model.encoder\n",
    "classifier = ViT_Classifier(encoder, num_classes=4)\n",
    "classifier.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "if custom_opt == \"prodigy\":\n",
    "    optimizer = Prodigy(classifier.parameters())\n",
    "elif custom_opt == \"adamW\":\n",
    "    optimizer = AdamW(classifier.parameters(), lr=learn_rate)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx, 0]\n",
    "        img_path = f'{fine_tune_path}/{img_name}' ##### MAN KAN ÆNDRE PATH HER ift /class_0\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = torch.tensor(self.df.iloc[idx, 2], dtype=torch.long)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "    def get_labels(self):\n",
    "        label = torch.tensor(self.df.iloc[:, 2].tolist(), dtype=torch.long)\n",
    "        return label\n",
    "    \n",
    "def init_dataloaders():\n",
    "        #DATA LABELS\n",
    "    df = pd.read_csv('img_labels_ALL.csv')\n",
    "\n",
    "    # Remove class 4 (images lablelled as bad examples)?\n",
    "    if use_class4 == False:\n",
    "        df = df[df['score'] != 4.0]\n",
    "    #df.head() #Sanity check\n",
    "\n",
    "    # Only use images that exist in the directory\n",
    "    image_folder = fine_tune_path\n",
    "    image_exists = df['img'].apply(lambda x: os.path.isfile(os.path.join(image_folder, x))) \n",
    "    filtered_df = df[image_exists]\n",
    "    print(f\"Original DataFrame size: {len(df)}, Filtered DataFrame size: {len(filtered_df)}\") #Sanity check\n",
    "    df = filtered_df\n",
    "\n",
    "    # Stratify/balance classes across splits\n",
    "    labels = df['score'].values\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=labels)\n",
    "    train_labels = train_df['score'].values\n",
    "    train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_labels)\n",
    "\n",
    "    # Create data loaders for training and validation sets\n",
    "    test_data = CustomDataset(test_df, transform)\n",
    "    train_data = CustomDataset(train_df, transform)\n",
    "    val_data = CustomDataset(val_df, transform)\n",
    "\n",
    "    #More workers for GPU/lambda\n",
    "    if torch.cuda.is_available():\n",
    "        num_workers_local = 0\n",
    "        test_loader = DataLoader(test_data, batch_size=batch_size, num_workers=num_workers_local)\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, sampler=ImbalancedDatasetSampler(train_data))#, num_workers=num_workers_local)\n",
    "        val_loader = DataLoader(val_data, batch_size=batch_size, num_workers=num_workers_local)\n",
    "    else:\n",
    "        test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, sampler=ImbalancedDatasetSampler(train_data))\n",
    "        val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "    return train_loader, val_loader, test_loader, test_data\n",
    "\n",
    "train_loader, val_loader, test_loader, test_data = init_dataloaders()\n",
    "from datetime import datetime\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# Format the datetime string to exclude the year\n",
    "formatted_datetime = now.strftime(\"%m-%d_%H-%M\")\n",
    "\n",
    "# Use the formatted datetime in the path\n",
    "writer = SummaryWriter(os.path.join(\"logs\", \"MES\", f\"{formatted_datetime}_{run_name}\"))\n",
    "\n",
    "#writer = SummaryWriter(os.path.join(\"logs\", \"MES\", \"MES_fine_tune\"))\n",
    "\n",
    "if fine_tuning == True:\n",
    "    best_val_loss = float('inf')\n",
    "    step_count = 0\n",
    "    best_epoch_was = 0\n",
    "    for epoch in range(num_fine_epochs):\n",
    "        \n",
    "        losses = []\n",
    "        pbar = tqdm(iter(train_loader))\n",
    "        classifier.train() # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        for images, labels in pbar:\n",
    "            # Move data to device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = classifier(images)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            pbar.set_postfix({'step loss': loss.item(), 'epoch': epoch}, refresh=False)\n",
    "            writer.add_scalar('Fine_tune_loss/train-step', loss.item(), global_step=step_count)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            step_count += 1\n",
    "            # Update running loss\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        # Compute average loss over the epoch\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        writer.add_scalar('Fine_tune_loss/train-epoch', epoch_loss, global_step=step_count)\n",
    "        \n",
    "        # Validation loop\n",
    "        classifier.eval() # Set the model to evaluation mode\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = classifier(images)\n",
    "                val_loss = criterion(outputs, labels)\n",
    "                running_val_loss += val_loss.item() * images.size(0)\n",
    "        \n",
    "        # Compute average validation loss over the epoch\n",
    "        val_epoch_loss = running_val_loss / len(val_loader.dataset)\n",
    "        writer.add_scalar('Fine_tune_loss/val-epoch', val_epoch_loss, global_step=step_count)\n",
    "\n",
    "        #save model after each epoch\n",
    "        torch.save({\n",
    "                    'model_state_dict': classifier.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss.item(),\n",
    "                    }, f'C:\\\\Users\\\\alx\\\\Downloads\\\\mdl_ckpt\\\\mes_m-{mask_ratio}\\\\mes_last.pth')\n",
    "        \n",
    "        #save as best if best\n",
    "        if val_epoch_loss < best_val_loss:\n",
    "            best_epoch_was = epoch\n",
    "            best_val_loss = val_epoch_loss\n",
    "            torch.save({\n",
    "                    'model_state_dict': classifier.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss.item(),\n",
    "                    'best_epoch_was': best_epoch_was,\n",
    "                    }, f'C:\\\\Users\\\\alx\\\\Downloads\\\\mdl_ckpt\\\\mes_m-{mask_ratio}\\\\mes_best.pth')\n",
    "\n",
    "\n",
    "    print(f\"Fine-tuning complete. Best epoch was {best_epoch_was}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load fine tuned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_fine_tuned:\n",
    "    import torch\n",
    "    import os\n",
    "    try:\n",
    "        step_count\n",
    "    except NameError:\n",
    "        step_count = 190000 // batch_size * 64\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(load_prev_fine_model_path)\n",
    "    encoder = MAE_Encoder(image_size=dim, mask_ratio=mask_ratio, emb_dim=emb_dims) #xxx\n",
    "    # Load the model state dict\n",
    "    classifier = ViT_Classifier(encoder).to(device)#\n",
    "    classifier.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Load the optimizer state dict\n",
    "    if custom_opt == \"prodigy\":\n",
    "        optimizer = Prodigy(classifier.parameters())\n",
    "    #optimizer.load_state_dict(checkpoint['optimizer_state_dict']) #do we need this?\n",
    "    elif custom_opt == \"adamW\":\n",
    "        optimizer = AdamW(classifier.parameters(), lr=learn_rate)\n",
    "    # Load the loss\n",
    "    loss = checkpoint['loss']\n",
    "    try:\n",
    "        best_epoch_was = checkpoint['best_epoch_was']\n",
    "    except KeyError:\n",
    "        best_epoch_was = num_fine_epochs\n",
    "    \n",
    "    \n",
    "    print(f\"Model loaded with loss: {loss}, where loaded epoch state was {best_epoch_was}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval fine tuned calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fine_tune_eval == True:    \n",
    "    #from sklearn.metrics import classification_report\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    def evaluate_single_sample(i):\n",
    "        # Get the sample from the dataset\n",
    "        sample, true_label = test_data[i]\n",
    "        \n",
    "        # Move the sample to the same device as the model\n",
    "        sample = sample.to(device)\n",
    "        \n",
    "        # Pass the sample through the model\n",
    "        with torch.no_grad():\n",
    "            prediction = classifier(sample.unsqueeze(0))  # Unsqueeze to add batch dimension\n",
    "\n",
    "        class_probabilities = prediction[0]\n",
    "\n",
    "\n",
    "        _, predicted_class = torch.max(class_probabilities, dim=0)\n",
    "\n",
    "        # Move the predicted_class back to CPU for further operations\n",
    "        predicted_class = predicted_class.to('cpu')\n",
    "        \n",
    "\n",
    "        \n",
    "        # Return the true label and the predicted class\n",
    "        return true_label.item(), predicted_class.item()\n",
    "\n",
    "    # Ensure the model is in evaluation mode\n",
    "    classifier.eval()\n",
    "\n",
    "    # Shared lists to store true labels and predicted classes\n",
    "    true_labels_list = []\n",
    "    predicted_classes_list = []\n",
    "\n",
    "    # Evaluate all samples sequentially\n",
    "    for i in tqdm(range(len(test_data)), desc='Evaluating'):\n",
    "        true_label, predicted_class = evaluate_single_sample(i)\n",
    "        true_labels_list.append(true_label)\n",
    "        predicted_classes_list.append(predicted_class)\n",
    "\n",
    "    # Assuming `true_labels` and `predicted_labels` are already defined\n",
    "    f1 = f1_score(true_labels_list, predicted_classes_list, average='weighted')\n",
    "    accuracy = accuracy_score(true_labels_list, predicted_classes_list)\n",
    "    precision = precision_score(true_labels_list, predicted_classes_list, average='weighted')\n",
    "    recall = recall_score(true_labels_list, predicted_classes_list, average='weighted')\n",
    "    cm = confusion_matrix(true_labels_list, predicted_classes_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Num params in clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in classifier.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval fine tune plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fine_tune_eval == True:   \n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    def format_number(num):\n",
    "        if num >= 1000000:\n",
    "            return f\"{num // 1000000}M\"\n",
    "        elif num >= 1000:\n",
    "            return f\"{num // 1000}K\"\n",
    "        else:\n",
    "            return str(num)\n",
    "    \n",
    "    # Create a figure and a 2x1 grid of subplots\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "    # Confusion matrix subplot\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axs[0])\n",
    "    axs[0].set_title('Confusion Matrix')\n",
    "    axs[0].set_xlabel('Predicted')\n",
    "    axs[0].set_ylabel('True')\n",
    "\n",
    "    # F1 Score, Precision, Recall, and Accuracy subplot\n",
    "    num_train_samples = len(dataloader)*batch_size\n",
    "    num_train_samples = format_number(num_train_samples)\n",
    "    axs[1].set_title(f'Optimizer = {custom_opt}, batch size: {batch_size} \\n MAE pretrained on {num_train_samples} training images, for: {num_epochs}e, fine tuned for: {num_fine_epochs}e, \\n masking ratio: {mask_ratio}, model has {total_params:,.0f} num params. \\n Model used: {model_used} (val loss); if best => converged on epoch {best_epoch_was}')\n",
    "    axs[1].text(0.5, 0.8, f'F1 Score: {f1:.2f}', horizontalalignment='center', verticalalignment='center', fontsize=14)\n",
    "    axs[1].text(0.5, 0.4, f'Precision: {precision:.2f}', horizontalalignment='center', verticalalignment='center', fontsize=14)\n",
    "    axs[1].text(0.5, 0.2, f'Recall: {recall:.2f}', horizontalalignment='center', verticalalignment='center', fontsize=14)\n",
    "    axs[1].text(0.5, 0.6, f'Accuracy: {accuracy:.2f}', horizontalalignment='center', verticalalignment='center', fontsize=14)\n",
    "    axs[1].axis('off')\n",
    "    \n",
    "    fig.suptitle(f'MES evaluation', fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./plots/eval_plot-{eval_plot_name}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total runtime of this run: {total_time//60//60} hrs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "\n",
    "# Play a sound\n",
    "winsound.PlaySound(\"SystemExit\", winsound.SND_ALIAS)\n",
    "winsound.PlaySound(\"SystemExit\", winsound.SND_ALIAS)\n",
    "winsound.PlaySound(\"SystemExit\", winsound.SND_ALIAS)\n",
    "winsound.PlaySound(\"SystemExit\", winsound.SND_ALIAS)\n",
    "winsound.PlaySound(\"SystemExit\", winsound.SND_ALIAS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
