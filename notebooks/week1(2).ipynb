{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ML imports\n",
    "# import pandas as pd #circular import?\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "# from torchvision import transforms\n",
    "# from PIL import Image\n",
    "# import timm\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from prodigyopt import Prodigy\n",
    "# from torchsampler import ImbalancedDatasetSampler\n",
    "#import lightning as L\n",
    "\n",
    "\n",
    "# #eval imports\n",
    "# from sklearn.metrics import classification_report\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# #Logging imports\n",
    "# import csv\n",
    "# import os\n",
    "# import datetime\n",
    "# from pathlib import Path\n",
    "\n",
    "# #Nice to have imports\n",
    "# from pathlib import Path\n",
    "# #from tqdm import tqdm #only for .py world\n",
    "# from mac_alerts import alerts\n",
    "# from tqdm.notebook import tqdm, trange\n",
    "# import wandb\n",
    "\n",
    "# wandb.init(project='BA1', entity='alexandermittet')\n",
    "\n",
    "# print(torch.__version__,'hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "single_shot = False\n",
    "learning_rate = 0.001 #Doesn't matter when using prodigy optimizer\n",
    "epochs = 50 #Max_limit\n",
    "batch_size = 64\n",
    "model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=5) # Get model architecture\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#weight_decay = 0.01  # Adjust based on your problem, 0 by default\n",
    "#d_coef = 0.5  # Adjust to force a smaller estimate of the learning rate, default is 1.0\n",
    "#optimizer = Prodigy(model.parameters(), lr=1., weight_decay=weight_decay, d_coef=d_coef)\n",
    "#optimizer = Prodigy(model.parameters())\n",
    "\n",
    "\n",
    "# If you're using a GPU, you might need to move the model to the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Dynamically update W&B configuration\n",
    "wandb.config.update({\n",
    "    \"single_shot\": single_shot,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size,  # Assuming this is how you access batch size\n",
    "    \"learning_rate\": optimizer.param_groups[0]['lr'],  # Dynamically get the learning rate from optimizer\n",
    "    \"optimizer\": optimizer.__class__.__name__,  # Dynamically get the optimizer class name\n",
    "    \"model_architecture\": str(model),\n",
    "    # Include any other dynamic parameters here\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame size: 2561, Filtered DataFrame size: 2556\n"
     ]
    }
   ],
   "source": [
    "#DATA LABELS\n",
    "#import pandas as pd\n",
    "df = pd.read_csv('img_labels_ALL.csv')\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "image_folder = 'img'  # The folder where your images are stored\n",
    "\n",
    "# Check if each image exists and create a mask\n",
    "image_exists = df['img'].apply(lambda x: os.path.isfile(os.path.join(image_folder, x)))\n",
    "\n",
    "# Filter the DataFrame using the mask\n",
    "filtered_df = df[image_exists]\n",
    "\n",
    "print(f\"Original DataFrame size: {len(df)}, Filtered DataFrame size: {len(filtered_df)}\")\n",
    "\n",
    "df = filtered_df\n",
    "# Now 'filtered_df' contains only the rows for which the images exist.\n",
    "# You can proceed with using 'filtered_df' for your dataset.\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx, 0]\n",
    "        img_path = f'img/{img_name}'\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = torch.tensor(self.df.iloc[idx, 2], dtype=torch.long)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "    def get_labels(self):\n",
    "        label = torch.tensor(self.df.iloc[:, 2].tolist(), dtype=torch.long)\n",
    "        return label\n",
    "\n",
    "# Define the transformations\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224,   224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485,   0.456,   0.406], std=[0.229,   0.224,   0.225]),\n",
    "# ])\n",
    "\n",
    "transform = timm.data.create_transform(\n",
    "    **timm.data.resolve_data_config(model.pretrained_cfg))\n",
    "\n",
    "# WEIGHTED RANDOM SAMPLER\n",
    "\n",
    "# score_counts = df['score'].value_counts()\n",
    "# percentage = score_counts / len(df) * 100\n",
    "# print(score_counts, percentage)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "score\n",
    "4.0    823\n",
    "0.0    822\n",
    "1.0    452\n",
    "2.0    351\n",
    "3.0    113\n",
    "Name: count, dtype: int64 score\n",
    "4.0    32.135884\n",
    "0.0    32.096837\n",
    "1.0    17.649356\n",
    "2.0    13.705584\n",
    "3.0     4.412339\n",
    "Name: count, dtype: float64\n",
    "'''\n",
    "\n",
    "# TEST SET\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "test_data = CustomDataset(test_df, transform)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# VALIDATION SET + TRAINING SET\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "#SAMPLER\n",
    "# train_class_counts = train_df['score'].value_counts().sort_index()\n",
    "# print(f'train_class_counts: {train_class_counts}')\n",
    "# train_class_counts = torch.tensor(train_class_counts.values, dtype=torch.float)\n",
    "#sampler = WeightedRandomSampler(weights = train_class_counts, num_samples = len(train_df), replacement=True) #JEG KAN ikke få til at virke. Den sampler de samme 4 billeder hele tiden... jeg kan faktisk heller ikke få lov at sige replacement = False\n",
    "\n",
    "\n",
    "# Create data loaders for training and validation sets\n",
    "train_data = CustomDataset(train_df, transform)\n",
    "val_data = CustomDataset(val_df, transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, sampler=ImbalancedDatasetSampler(train_data))\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Define loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #DEBUGGING TRAIN LOADER / SAMPLER\n",
    "# # Antag, at du har en DataLoader kaldet 'train_loader'\n",
    "# iterator = iter(train_loader)\n",
    "# # Vis det første billede i batchen\n",
    "# import matplotlib.pyplot as plt\n",
    "# Hent den næste batch af data\n",
    "# data, labels = next(iterator)\n",
    "# # Display the first  10 images in the batch\n",
    "# for i in range(10):  # Adjust the range to display more or fewer images\n",
    "#     plt.figure(figsize=(2,  2))  # Set the figure size to  5x5 inches\n",
    "#     plt.imshow(data[i].permute(1,  2,  0))  # Permute for at  ændre dimensionerne til HWC\n",
    "#     plt.title(f\"Label: {labels[i]}\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP\"\n",
    "\n",
    "# Define a function to evaluate the model on the validation set\n",
    "def evaluate_on_validation_set(model, val_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move inputs and labels to the device\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Define paths for saving models relative to the current working directory\n",
    "model_dir = os.path.join(cwd, \"models\")\n",
    "best_model_path = os.path.join(model_dir, \"best_model.pt\")\n",
    "last_model_path = os.path.join(model_dir, \"last_model.pt\")\n",
    "\n",
    "best_val_loss = float('inf')  # Initialize best validation loss to infinity\n",
    "best_val_accuracy = 0.0  # Alternatively, if optimizing for accuracy\n",
    "model_save_path = 'best_model.pth'  # Define where to save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING LOOP\n",
    "# Wrap both loops with tqdm for progress visualization\n",
    "for epoch in trange(epochs, desc=\"Training Epochs\"):\n",
    "    # Traning phase\n",
    "    model.train()  # Set model to training mode\n",
    "    train_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\", leave=False)\n",
    "    for i, data in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch + 1}\", leave=False)):\n",
    "        inputs, labels = [d.to(device) for d in data]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the progress bar with the current batch's loss\n",
    "        pbar.set_postfix({'Batch Loss': loss.item()})\n",
    "        train_loss += loss.item()\n",
    "    # Average loss for the epoch\n",
    "    train_loss /= len(train_loader)\n",
    "    tqdm.write(f\"Epoch {epoch + 1}: Avg. Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # No gradients needed for validation\n",
    "        for inputs, labels in tqdm(val_loader, desc=f\"Epoch {epoch + 1}/Validation\", leave=False):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = correct / total\n",
    "\n",
    "    # Use tqdm.write to avoid interference with the progress bars\n",
    "    tqdm.write(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Wandb log epoch metrics\n",
    "    wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"val_loss\": val_loss, \"val_accuracy\": val_accuracy})\n",
    "    \n",
    "\n",
    "    # Save model if validation loss decreased\n",
    "    if val_loss < best_val_loss:\n",
    "        tqdm.write(f\"Validation loss decreased ({best_val_loss:.4f} --> {val_loss:.4f}). Saving model...\")\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        \n",
    "wandb.finish()\n",
    "alerts.play_success()\n",
    "os.system('say \"Træning færdig\"'); print('\\a\\a\\a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parallel evaluations of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_single_sample(i):\n",
    "    # Get the sample from the dataset\n",
    "    sample, true_label = test_data[i]\n",
    "    \n",
    "    # Move the sample to the same device as the model\n",
    "    sample = sample.to(device)\n",
    "    \n",
    "    # Pass the sample through the model\n",
    "    with torch.no_grad():\n",
    "        prediction = model(sample.unsqueeze(0))  # Unsqueeze to add batch dimension\n",
    "    \n",
    "    # Convert the prediction to a class label\n",
    "    _, predicted_class = torch.max(prediction, dim=1)\n",
    "    \n",
    "    # Move the predicted_class back to CPU for further operations\n",
    "    predicted_class = predicted_class.to('cpu')\n",
    "    \n",
    "    # Return the true label and the predicted class\n",
    "    return true_label.item(), predicted_class.item()\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Shared lists to store true labels and predicted classes\n",
    "true_labels_list = []\n",
    "predicted_classes_list = []\n",
    "\n",
    "# Create a thread pool executor\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    # Evaluate all samples in parallel and collect results\n",
    "    results = list(tqdm(executor.map(evaluate_single_sample, range(len(test_data))), total=len(test_data), desc='Evaluating'))\n",
    "\n",
    "# Collect the true labels and predicted classes from the results\n",
    "for true_label, predicted_class in results:\n",
    "    true_labels_list.append(true_label)\n",
    "    predicted_classes_list.append(predicted_class)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(true_labels_list, predicted_classes_list)\n",
    "\n",
    "# Print the classification report\n",
    "print(report)\n",
    "\n",
    "# Play a success sound\n",
    "from mac_alerts import alerts\n",
    "import os\n",
    "alerts.play_success()\n",
    "os.system('say \"Evaluering færdig\"') \n",
    "print('\\a\\a\\a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
