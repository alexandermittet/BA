{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# wandb.init(project=\"BA-MAE\", entity=\"alexandermittet\")\n",
    "\n",
    "########### PATHS ############\n",
    "pretrain_path =     r'C:\\Users\\alx\\Downloads\\BA_data\\frames'\n",
    "#for t-sne:\n",
    "pretrain_path_imgs = '/Users/alexandermittet/Library/CloudStorage/SeaDrive-almi(seafile.erda.dk) (14.03.2024 13.32)/My Libraries/BA_data/frames/class_0'\n",
    "val_path =          r'C:\\Users\\alx\\Downloads\\BA_data\\val_frames'\n",
    "#for t-sne:\n",
    "val_path_imgs =     \"/Users/alexandermittet/Library/CloudStorage/SeaDrive-almi(seafile.erda.dk) (14.03.2024 13.32)/My Libraries/BA_data/validation_frames/class_0\"\n",
    "fine_tune_path =    'C:\\\\Users\\\\alx\\\\Downloads\\\\img\\\\img'\n",
    "\n",
    "########### HYPER PARAMETERS ############\n",
    "### MODEL SIZE\n",
    "emb_dims =          192#192 #skal ku duvuderes med num_heads som er 4 \n",
    "                    #85M params i TIMM classifiers, så den har vi smags til at 190*4 passer med 12 layers\n",
    "encoder_layers =    12#12 i timm\n",
    "\n",
    "\n",
    "### Pretraining\n",
    "load_prev_model =   False\n",
    "load_prev_model_path = r\"C:\\Users\\alx\\Downloads\\mdl_ckpt\\mae_m-0.75\\mae_best_e-328_7.5M.pth\"\n",
    "training =          False\n",
    "num_epochs =        0\n",
    "dim =               224\n",
    "batch_size =        32\n",
    "mask_ratio =        0.75\n",
    "# wandb.config.mask_ratio = mask_ratio\n",
    "# wandb.config.batch_size = batch_size\n",
    "\n",
    "### Fine-tuning\n",
    "fine_tuning =       True\n",
    "load_fine_tuned =   False\n",
    "fine_tune_eval =    True\n",
    "num_fine_epochs =   50 #overfitter efter 4 faktisk, men vi gemmer også best\n",
    "\n",
    "eval_plot_name =    f\"Baseline_e-{num_fine_epochs}_PreTrain_MAE_e-{num_epochs}_mask-{mask_ratio}_params-5.5M\"\n",
    "load_prev_fine_model_path = f'C:\\\\Users\\\\alx\\\\Downloads\\\\mdl_ckpt\\\\mes_m-{mask_ratio}\\\\baseline_mes_best_5.5M.pth'\n",
    "\n",
    "### T-sne feature plotting\n",
    "tsne_plotting =     False\n",
    "tsne_target =       \"validation images\"\n",
    "plot_single_image = True\n",
    "plot_16_images =    True\n",
    "\n",
    "### Misc\n",
    "get_num_params =    True\n",
    "break_after_num_steps = -1\n",
    "use_class4 =        False\n",
    "\n",
    "run_name =          f\"e-{num_epochs}_m-{mask_ratio}_fine-e-{num_fine_epochs}_bs-{batch_size}_emb_dim-{emb_dims}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "#import torch.optim as optim #bruger prodigy\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from prodigyopt import Prodigy\n",
    "import lightning as L\n",
    "# import wandb\n",
    "\n",
    "from einops import repeat, rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from timm.models.layers import trunc_normal_\n",
    "from timm.models.vision_transformer import Block\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((dim, dim)), # Resize the image to #32x32\n",
    "    transforms.ToTensor(), # Convert the image to a PyTorch tensor\n",
    "])\n",
    "\n",
    "#dataset = CustomFramesDataset(root_dir=frames_path, transform=transform)\n",
    "dataset = ImageFolder(\n",
    "        pretrain_path,\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "val_dataset = ImageFolder(\n",
    "        val_path,\n",
    "        transform=transform,\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model architecture + masking fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ViT Model + functions\n",
    "class MAE_Encoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_size=dim,\n",
    "                 patch_size=16,\n",
    "                 emb_dim=emb_dims,\n",
    "                 num_layer=encoder_layers,\n",
    "                 num_head=4,\n",
    "                 mask_ratio=mask_ratio,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.cls_token = torch.nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
    "        self.pos_embedding = torch.nn.Parameter(torch.zeros((image_size // patch_size) ** 2, 1, emb_dim))\n",
    "        self.shuffle = PatchShuffle(mask_ratio)\n",
    "\n",
    "        self.patchify = torch.nn.Conv2d(3, emb_dim, patch_size, patch_size)\n",
    "\n",
    "        self.transformer = torch.nn.Sequential(*[Block(emb_dim, num_head) for _ in range(num_layer)])\n",
    "\n",
    "        self.layer_norm = torch.nn.LayerNorm(emb_dim)\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        trunc_normal_(self.pos_embedding, std=.02)\n",
    "\n",
    "    def forward(self, img):\n",
    "        patches = self.patchify(img)\n",
    "        patches = rearrange(patches, 'b c h w -> (h w) b c')\n",
    "        \n",
    "        # Calculate the number of patches\n",
    "        num_patches = patches.shape[0]\n",
    "        \n",
    "\n",
    "        \n",
    "        patches = patches + self.pos_embedding\n",
    "\n",
    "        patches, forward_indexes, backward_indexes = self.shuffle(patches)\n",
    "\n",
    "        patches = torch.cat([self.cls_token.expand(-1, patches.shape[1], -1), patches], dim=0)\n",
    "        patches = rearrange(patches, 't b c -> b t c')\n",
    "        features = self.layer_norm(self.transformer(patches))\n",
    "        features = rearrange(features, 'b t c -> t b c')\n",
    "\n",
    "        return features, backward_indexes\n",
    "\n",
    "class MAE_Decoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_size=dim,\n",
    "                 patch_size=16,\n",
    "                 emb_dim=192,\n",
    "                 num_layer=4,\n",
    "                 num_head=3,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.mask_token = torch.nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
    "        self.pos_embedding = torch.nn.Parameter(torch.zeros((image_size // patch_size) ** 2 + 1, 1, emb_dim))\n",
    "\n",
    "        self.transformer = torch.nn.Sequential(*[Block(emb_dim, num_head) for _ in range(num_layer)])\n",
    "\n",
    "        self.head = torch.nn.Linear(emb_dim, 3 * patch_size ** 2)\n",
    "        self.patch2img = Rearrange('(h w) b (c p1 p2) -> b c (h p1) (w p2)', p1=patch_size, p2=patch_size, h=image_size//patch_size)\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        trunc_normal_(self.mask_token, std=.02)\n",
    "        trunc_normal_(self.pos_embedding, std=.02)\n",
    "\n",
    "    def forward(self, features, backward_indexes):\n",
    "        T = features.shape[0]\n",
    "        backward_indexes = torch.cat([torch.zeros(1, backward_indexes.shape[1]).to(backward_indexes), backward_indexes + 1], dim=0)\n",
    "        features = torch.cat([features, self.mask_token.expand(backward_indexes.shape[0] - features.shape[0], features.shape[1], -1)], dim=0)\n",
    "        features = take_indexes(features, backward_indexes)\n",
    "        features = features + self.pos_embedding\n",
    "\n",
    "        features = rearrange(features, 't b c -> b t c')\n",
    "        features = self.transformer(features)\n",
    "        features = rearrange(features, 'b t c -> t b c')\n",
    "        features = features[1:] # remove global feature\n",
    "\n",
    "        patches = self.head(features)\n",
    "        mask = torch.zeros_like(patches)\n",
    "        mask[T-1:] = 1\n",
    "        mask = take_indexes(mask, backward_indexes[1:] - 1)\n",
    "        img = self.patch2img(patches)\n",
    "        mask = self.patch2img(mask)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "class MAE_ViT(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_size=dim,\n",
    "                 patch_size=16,\n",
    "                 emb_dim=emb_dims,#192,\n",
    "                 encoder_layer=encoder_layers,#12,\n",
    "                 encoder_head=4,\n",
    "                 decoder_layer=4,\n",
    "                 decoder_head=4,\n",
    "                 mask_ratio=mask_ratio,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = MAE_Encoder(image_size, patch_size, emb_dim, encoder_layer, encoder_head, mask_ratio)\n",
    "        self.decoder = MAE_Decoder(image_size, patch_size, emb_dim, decoder_layer, decoder_head)\n",
    "\n",
    "    def forward(self, img):\n",
    "        features, backward_indexes = self.encoder(img)\n",
    "        predicted_img, mask = self.decoder(features,  backward_indexes)\n",
    "        return predicted_img, mask\n",
    "\n",
    "class ViT_Classifier(torch.nn.Module):\n",
    "    def __init__(self, encoder : MAE_Encoder, num_classes=4) -> None:\n",
    "        super().__init__()\n",
    "        self.cls_token = encoder.cls_token\n",
    "        self.pos_embedding = encoder.pos_embedding\n",
    "        self.patchify = encoder.patchify\n",
    "        self.transformer = encoder.transformer\n",
    "        self.layer_norm = encoder.layer_norm\n",
    "        self.head = torch.nn.Linear(self.pos_embedding.shape[-1], num_classes)\n",
    "\n",
    "    def forward(self, img):\n",
    "        patches = self.patchify(img)\n",
    "        patches = rearrange(patches, 'b c h w -> (h w) b c')\n",
    "        patches = patches + self.pos_embedding\n",
    "        patches = torch.cat([self.cls_token.expand(-1, patches.shape[1], -1), patches], dim=0)\n",
    "        patches = rearrange(patches, 't b c -> b t c')\n",
    "        features = self.layer_norm(self.transformer(patches))\n",
    "        features = rearrange(features, 'b t c -> t b c')\n",
    "        logits = self.head(features[0])\n",
    "        return logits\n",
    "    \n",
    "\n",
    "class PatchShuffle(torch.nn.Module):\n",
    "    def __init__(self, ratio) -> None:\n",
    "        super().__init__()\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def forward(self, patches : torch.Tensor):\n",
    "        T, B, C = patches.shape\n",
    "        remain_T = int(T * (1 - self.ratio))\n",
    "\n",
    "        indexes = [random_indexes(T) for _ in range(B)]\n",
    "        forward_indexes = torch.as_tensor(np.stack([i[0] for i in indexes], axis=-1), dtype=torch.long).to(patches.device)\n",
    "        backward_indexes = torch.as_tensor(np.stack([i[1] for i in indexes], axis=-1), dtype=torch.long).to(patches.device)\n",
    "\n",
    "        patches = take_indexes(patches, forward_indexes)\n",
    "        patches = patches[:remain_T]\n",
    "\n",
    "        return patches, forward_indexes, backward_indexes\n",
    "    \n",
    "def random_indexes(size : int):\n",
    "    forward_indexes = np.arange(size)\n",
    "    np.random.shuffle(forward_indexes)\n",
    "    backward_indexes = np.argsort(forward_indexes)\n",
    "    return forward_indexes, backward_indexes\n",
    "\n",
    "def take_indexes(sequences, indexes):\n",
    "    return torch.gather(sequences, 0, repeat(indexes, 't b -> t b c', c=sequences.shape[-1]))\n",
    "\n",
    "def mask_image(image, mask_size):\n",
    "    mask = torch.ones_like(image)\n",
    "    mask[:, :mask_size, :mask_size] = 0\n",
    "    return image * mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load  pre-train model if it exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    import torch\n",
    "    import os\n",
    "\n",
    "    if os.path.exists(load_prev_model_path):\n",
    "        # Load the checkpoint\n",
    "        checkpoint = torch.load(load_prev_model_path)\n",
    "        \n",
    "        # Load the model state dict\n",
    "        model = MAE_ViT().to(device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # Load the optimizer state dict\n",
    "        optimizer = Prodigy(model.parameters())\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "        # Load the loss\n",
    "        loss = checkpoint['loss']\n",
    "        \n",
    "        print(f\"Model loaded with loss: {loss}\")\n",
    "    else:\n",
    "        model = MAE_ViT().to(device)\n",
    "        print(\"Model checkpoint not found. Init model from scratch.\")\n",
    "    return model, optimizer, loss\n",
    "\n",
    "if load_prev_model:\n",
    "    model, optimizer, loss = load_model()\n",
    "else:\n",
    "    model = MAE_ViT().to(device)\n",
    "    optimizer = Prodigy(model.parameters())\n",
    "    #loss er custom mse loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Num_params in MAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_num_params:\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total number of parameters: {total_params:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training:    #Tensorboard writer\n",
    "    from torchsampler import ImbalancedDatasetSampler\n",
    "    \n",
    "    from datetime import datetime\n",
    "    now = datetime.now()\n",
    "    # Format the datetime string to exclude the year\n",
    "    formatted_datetime = now.strftime(\"%m-%d_%H-%M\")\n",
    "    # Use the formatted datetime in the path\n",
    "    writer = SummaryWriter(os.path.join(\"logs\", \"mae\", f\"{formatted_datetime}_{run_name}\"))\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, sampler=ImbalancedDatasetSampler(dataset)) #sampler=cycle_sampler\n",
    "    criterion = torch.nn.MSELoss() #pas\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    step_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        pbar = tqdm(iter(dataloader))\n",
    "        for img, label in pbar:\n",
    "            step_count += 1\n",
    "            img = img.to(device)\n",
    "            predicted_img, mask = model(img)\n",
    "            loss = (\n",
    "                    torch.mean((predicted_img - img) ** 2 * mask) / mask_ratio\n",
    "                )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            pbar.set_postfix({'step loss': loss.item()}, refresh=False)\n",
    "            writer.add_scalar(\"mae_step_loss\", loss.item(), global_step=step_count)\n",
    "            # wandb.log({\"Train Loss step\": loss})\n",
    "\n",
    "            if break_after_num_steps > 0: #set to -1 to disable early breaking\n",
    "                if step_count >= break_after_num_steps: #Save and quit\n",
    "                    # torch.save({\n",
    "                    #             'model_state_dict': model.state_dict(),\n",
    "                    #             'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    #             'loss': loss.item(),\n",
    "                    #             }, './models/mae_checkpoint_break.pth')\n",
    "                    break\n",
    "        \n",
    "        #For every epoch:\n",
    "        avg_loss = sum(losses) / len(losses)\n",
    "        writer.add_scalar(\"mae_epoch_loss\", avg_loss, global_step=step_count)\n",
    "        print(f\"In epoch {epoch}, average training loss is {avg_loss}.\")\n",
    "        # wandb.log({\"Train Loss epoch avg\": avg_loss})\n",
    "\n",
    "        \"\"\" visualize the first 16 predicted images on val dataset\"\"\"\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_img = torch.stack([val_dataset[i][0] for i in range(16)])\n",
    "            val_img = val_img.to(device)\n",
    "            predicted_val_img, mask = model(val_img)\n",
    "            predicted_val_img = predicted_val_img * mask + val_img * (1 - mask)\n",
    "            img = torch.cat(\n",
    "                [val_img * (1 - mask), predicted_val_img, val_img], dim=0\n",
    "            )\n",
    "            img = rearrange(\n",
    "                img, \"(v h1 w1) c h w -> c (h1 h) (w1 v w)\", w1=2, v=3\n",
    "            )\n",
    "            writer.add_image(\"mae_image\", (img + 1) / 2, global_step=epoch)\n",
    "            # transform = transforms.ToPILImage()\n",
    "            # pil_img = transform(img)\n",
    "            # wandb_img = wandb.Image(pil_img)\n",
    "            # wandb.log({\"16 val mae_images\": wandb_img})\n",
    "\n",
    "        \"\"\" save model \"\"\"\n",
    "        # base_path = \"C:\\\\Users\\\\alx\\\\Downloads\\\\mdl_ckptmae_checkpoint_epoch\"\n",
    "        # torch.save({\n",
    "        #             'model_state_dict': model.state_dict(),\n",
    "        #             'optimizer_state_dict': optimizer.state_dict(),\n",
    "        #             'loss': loss.item(),\n",
    "        #             }, os.path.join(base_path, f\"-{epoch}_{step_count}_mask-{mask_ratio}.pth\"))\n",
    "        if avg_loss < best_loss:\n",
    "            # Update the best loss\n",
    "            best_loss = avg_loss\n",
    "            \n",
    "            # Construct the filename with the current loss included\n",
    "            filename = f\"mae_best.pth\"\n",
    "        else:\n",
    "            filename = \"mae_last.pth\"\n",
    "        # Construct the full path using os.path.join\n",
    "        base_path = f\"C:\\\\Users\\\\alx\\\\Downloads\\\\mdl_ckpt\\\\mae_m-{mask_ratio}\"\n",
    "        full_path = os.path.join(base_path, filename)\n",
    "        \n",
    "        # Save the model\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss.item(),\n",
    "        }, full_path)\n",
    "            \n",
    "        print(f\"Model saved with avg loss {avg_loss:.4f} at {full_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval 16 imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_16_images:\n",
    "    import matplotlib.pyplot as plt\n",
    "    val_img = torch.stack([val_dataset[i][0] for i in range(16)])\n",
    "    val_img = val_img.to(device)\n",
    "    predicted_val_img, mask = model(val_img)\n",
    "    predicted_val_img = predicted_val_img * mask + val_img * (1 - mask)\n",
    "    img = torch.cat([val_img * (1 - mask), predicted_val_img, val_img], dim=0)\n",
    "    img = rearrange(img, \"(v h1 w1) c h w -> (h1 h) (w1 v w) c\", w1=2, v=3)\n",
    "    img_np = img.cpu().detach().numpy()\n",
    "    min_val, max_val = np.min(img_np), np.max(img_np)\n",
    "    range_val = max_val - min_val\n",
    "\n",
    "    # Normalize the data\n",
    "    normalized_img_np = (img_np - min_val) / range_val\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.axis('off')\n",
    "    plt.title(f'MAE: \\n 16 first val imgs. MAE pretrained for: {num_epochs}e, masking: {mask_ratio} \\n MAE model has {total_params:,.0f}')\n",
    "    plt.imshow(normalized_img_np)\n",
    "    plt.savefig(f\"plots/mae_image_16x_e-{num_epochs}_m-{mask_ratio}_params-{total_params:,.0f}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-SNE current results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DO the work one time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_embeddings(input_path):\n",
    "    from torchvision import transforms\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "\n",
    "    import torch\n",
    "\n",
    "    #model, optimizer, loss = load_model() # we do this in another cell\n",
    "    # Extract the encoder\n",
    "    encoder = model.encoder\n",
    "    encoder.eval() # Set the model to evaluation mode\n",
    "\n",
    "    import os\n",
    "\n",
    "    # Specify the directory containing your images\n",
    "    image_dir = input_path\n",
    "    all_files = os.listdir(image_dir)\n",
    "    # Filter out image files (assuming .jpg and .png extensions)\n",
    "    image_files = [file for file in all_files if file.endswith(('.jpg', '.png'))]\n",
    "\n",
    "    # Construct the full paths to the image files\n",
    "    image_paths = [os.path.join(image_dir, file) for file in image_files]\n",
    "    # Load and preprocess images\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)), # Adjust size as needed\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "\n",
    "    from tqdm import tqdm # Import tqdm\n",
    "\n",
    "    # Extract features\n",
    "    features = []\n",
    "    for path in tqdm(image_paths, desc=\"Processing images\"): # Wrap image_paths with tqdm\n",
    "        img = Image.open(path)\n",
    "        img = transform(img).unsqueeze(0) # Add batch dimension\n",
    "        with torch.no_grad():\n",
    "            _, feature = encoder(img)\n",
    "        features.append(feature.squeeze().numpy())\n",
    "\n",
    "    # Stack features into a 2D array\n",
    "    features_array = np.vstack(features)\n",
    "\n",
    "    from sklearn.manifold import TSNE\n",
    "\n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    embedding = tsne.fit_transform(features_array)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    return embedding\n",
    "\n",
    "if tsne_plotting:\n",
    "    if tsne_target == \"training images\":\n",
    "        embedding = tsne_embeddings(pretrain_path_imgs)\n",
    "    elif tsne_target == \"validation images\":\n",
    "        embedding = tsne_embeddings(val_path_imgs)\n",
    "    else:\n",
    "        print(\"Invalid target for t-SNE plotting. Please choose either 'training images' or 'validation images'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just tsne plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:# Plot the 2D embedding\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(embedding[:, 0], embedding[:, 1])\n",
    "    plt.title('t-SNE visualization of image embeddings')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tsne with images on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne(input_path, which_type):\n",
    "    import os\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "    def getImage(path, zoom=.025):\n",
    "        return OffsetImage(plt.imread(path), zoom=zoom)\n",
    "\n",
    "    # Specify the directory containing the images\n",
    "    image_directory = input_path\n",
    "\n",
    "    # List all files in the directory\n",
    "    files = os.listdir(image_directory)\n",
    "\n",
    "    # Filter out the image files (assuming .jpg and .png extensions)\n",
    "    image_files = [os.path.join(image_directory, f) for f in files if f.endswith(('.jpg', '.png'))]\n",
    "\n",
    "    # Create a figure and axes\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Assuming 'embedding' is defined elsewhere in your code\n",
    "    for x0, y0, path in zip(embedding[:, 0], embedding[:, 1], image_files):\n",
    "        ab = AnnotationBbox(getImage(path), (x0, y0), frameon=False)\n",
    "        ax.add_artist(ab)\n",
    "\n",
    "    # Set the limits of the plot to ensure all images are visible\n",
    "    # You might need to adjust these limits based on the range of your embedding coordinates\n",
    "    ax.set_xlim(embedding[:, 0].min(), embedding[:, 0].max())\n",
    "    ax.set_ylim(embedding[:, 1].min(), embedding[:, 1].max())\n",
    "    from datetime import datetime\n",
    "    now = datetime.now()\n",
    "    timestamp = now.strftime(\"%y%m%d_%H%M\")\n",
    "    \n",
    "    title = f\" T-sne plot of feature embeddings of {which_type} images. Mask Ratio: {mask_ratio}\"\n",
    "\n",
    "    # Set the title of the plot\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Save the figure with the datetime stamp in the filename\n",
    "    plt.savefig(f'./plots/tsne_time-{timestamp}_mask-{mask_ratio}_type-{which_type}.png')\n",
    "    plt.show()\n",
    "\n",
    "if tsne_plotting:\n",
    "    if tsne_target == \"training images\":\n",
    "        plot_tsne(pretrain_path_imgs, 'training')\n",
    "    elif tsne_target == \"validation images\":\n",
    "        plot_tsne(val_path_imgs,'validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune model MES score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchsampler import ImbalancedDatasetSampler\n",
    "encoder = model.encoder\n",
    "classifier = ViT_Classifier(encoder, num_classes=4)\n",
    "classifier.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = Prodigy(classifier.parameters())\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx, 0]\n",
    "        img_path = f'{fine_tune_path}/{img_name}' ##### MAN KAN ÆNDRE PATH HER ift /class_0\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = torch.tensor(self.df.iloc[idx, 2], dtype=torch.long)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "    def get_labels(self):\n",
    "        label = torch.tensor(self.df.iloc[:, 2].tolist(), dtype=torch.long)\n",
    "        return label\n",
    "    \n",
    "def init_dataloaders():\n",
    "        #DATA LABELS\n",
    "    df = pd.read_csv('img_labels_ALL.csv')\n",
    "\n",
    "    # Remove class 4 (images lablelled as bad examples)?\n",
    "    if use_class4 == False:\n",
    "        df = df[df['score'] != 4.0]\n",
    "    #df.head() #Sanity check\n",
    "\n",
    "    # Only use images that exist in the directory\n",
    "    image_folder = fine_tune_path\n",
    "    image_exists = df['img'].apply(lambda x: os.path.isfile(os.path.join(image_folder, x))) \n",
    "    filtered_df = df[image_exists]\n",
    "    print(f\"Original DataFrame size: {len(df)}, Filtered DataFrame size: {len(filtered_df)}\") #Sanity check\n",
    "    df = filtered_df\n",
    "\n",
    "    # Stratify/balance classes across splits\n",
    "    labels = df['score'].values\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=labels)\n",
    "    train_labels = train_df['score'].values\n",
    "    train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_labels)\n",
    "\n",
    "    # Create data loaders for training and validation sets\n",
    "    test_data = CustomDataset(test_df, transform)\n",
    "    train_data = CustomDataset(train_df, transform)\n",
    "    val_data = CustomDataset(val_df, transform)\n",
    "\n",
    "    #More workers for GPU/lambda\n",
    "    if torch.cuda.is_available():\n",
    "        num_workers_local = 0\n",
    "        test_loader = DataLoader(test_data, batch_size=batch_size, num_workers=num_workers_local)\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, sampler=ImbalancedDatasetSampler(train_data))#, num_workers=num_workers_local)\n",
    "        val_loader = DataLoader(val_data, batch_size=batch_size, num_workers=num_workers_local)\n",
    "    else:\n",
    "        test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, sampler=ImbalancedDatasetSampler(train_data))\n",
    "        val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "    return train_loader, val_loader, test_loader, test_data\n",
    "\n",
    "train_loader, val_loader, test_loader, test_data = init_dataloaders()\n",
    "from datetime import datetime\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# Format the datetime string to exclude the year\n",
    "formatted_datetime = now.strftime(\"%m-%d_%H-%M\")\n",
    "\n",
    "# Use the formatted datetime in the path\n",
    "writer = SummaryWriter(os.path.join(\"logs\", \"MES\", f\"{formatted_datetime}_{run_name}\"))\n",
    "\n",
    "#writer = SummaryWriter(os.path.join(\"logs\", \"MES\", \"MES_fine_tune\"))\n",
    "\n",
    "if fine_tuning == True:\n",
    "    best_val_loss = float('inf')\n",
    "    step_count = 0\n",
    "    best_epoch_was = 0\n",
    "    for epoch in range(num_fine_epochs):\n",
    "        \n",
    "        losses = []\n",
    "        pbar = tqdm(iter(train_loader))\n",
    "        classifier.train() # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        for images, labels in pbar:\n",
    "            # Move data to device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = classifier(images)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            pbar.set_postfix({'step loss': loss.item(), 'epoch': epoch}, refresh=False)\n",
    "            writer.add_scalar('Fine_tune_loss/train-step', loss.item(), global_step=step_count)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            step_count += 1\n",
    "            # Update running loss\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        # Compute average loss over the epoch\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        writer.add_scalar('Fine_tune_loss/train-epoch', epoch_loss, global_step=step_count)\n",
    "        \n",
    "        # Validation loop\n",
    "        classifier.eval() # Set the model to evaluation mode\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = classifier(images)\n",
    "                val_loss = criterion(outputs, labels)\n",
    "                running_val_loss += val_loss.item() * images.size(0)\n",
    "        \n",
    "        # Compute average validation loss over the epoch\n",
    "        val_epoch_loss = running_val_loss / len(val_loader.dataset)\n",
    "        writer.add_scalar('Fine_tune_loss/val-epoch', val_epoch_loss, global_step=step_count)\n",
    "\n",
    "        #save model after each epoch\n",
    "        torch.save({\n",
    "                    'model_state_dict': classifier.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss.item(),\n",
    "                    }, f'C:\\\\Users\\\\alx\\\\Downloads\\\\mdl_ckpt\\\\mes_m-{mask_ratio}\\\\mes_last.pth')\n",
    "        \n",
    "        #save as best if best\n",
    "        if val_epoch_loss < best_val_loss:\n",
    "            best_epoch_was = epoch\n",
    "            best_val_loss = val_epoch_loss\n",
    "            torch.save({\n",
    "                    'model_state_dict': classifier.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss.item(),\n",
    "                    'best_epoch_was': best_epoch_was,\n",
    "                    }, f'C:\\\\Users\\\\alx\\\\Downloads\\\\mdl_ckpt\\\\mes_m-{mask_ratio}\\\\mes_best.pth')\n",
    "\n",
    "\n",
    "    print(f\"Fine-tuning complete. Best epoch was {best_epoch_was}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load fine tuned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_fine_tuned:\n",
    "    import torch\n",
    "    import os\n",
    "\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(load_prev_fine_model_path)\n",
    "    encoder = MAE_Encoder(image_size=dim, mask_ratio=mask_ratio, emb_dim=emb_dims) #xxx\n",
    "    # Load the model state dict\n",
    "    classifier = ViT_Classifier(encoder).to(device)#\n",
    "    classifier.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Load the optimizer state dict\n",
    "    optimizer = Prodigy(classifier.parameters())\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    # Load the loss\n",
    "    loss = checkpoint['loss']\n",
    "\n",
    "    best_epoch_was = checkpoint['best_epoch_was']\n",
    "    \n",
    "    print(f\"Model loaded with loss: {loss}, where best epoch was {best_epoch_was}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval fine tuned calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fine_tune_eval == True:    \n",
    "    #from sklearn.metrics import classification_report\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    def evaluate_single_sample(i):\n",
    "        # Get the sample from the dataset\n",
    "        sample, true_label = test_data[i]\n",
    "        \n",
    "        # Move the sample to the same device as the model\n",
    "        sample = sample.to(device)\n",
    "        \n",
    "        # Pass the sample through the model\n",
    "        with torch.no_grad():\n",
    "            prediction = classifier(sample.unsqueeze(0))  # Unsqueeze to add batch dimension\n",
    "\n",
    "        class_probabilities = prediction[0]\n",
    "\n",
    "\n",
    "        _, predicted_class = torch.max(class_probabilities, dim=0)\n",
    "\n",
    "        # Move the predicted_class back to CPU for further operations\n",
    "        predicted_class = predicted_class.to('cpu')\n",
    "        \n",
    "\n",
    "        \n",
    "        # Return the true label and the predicted class\n",
    "        return true_label.item(), predicted_class.item()\n",
    "\n",
    "    # Ensure the model is in evaluation mode\n",
    "    classifier.eval()\n",
    "\n",
    "    # Shared lists to store true labels and predicted classes\n",
    "    true_labels_list = []\n",
    "    predicted_classes_list = []\n",
    "\n",
    "    # Evaluate all samples sequentially\n",
    "    for i in tqdm(range(len(test_data)), desc='Evaluating'):\n",
    "        true_label, predicted_class = evaluate_single_sample(i)\n",
    "        true_labels_list.append(true_label)\n",
    "        predicted_classes_list.append(predicted_class)\n",
    "\n",
    "    # Assuming `true_labels` and `predicted_labels` are already defined\n",
    "    f1 = f1_score(true_labels_list, predicted_classes_list, average='weighted')\n",
    "    accuracy = accuracy_score(true_labels_list, predicted_classes_list)\n",
    "    precision = precision_score(true_labels_list, predicted_classes_list, average='weighted')\n",
    "    recall = recall_score(true_labels_list, predicted_classes_list, average='weighted')\n",
    "    cm = confusion_matrix(true_labels_list, predicted_classes_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Num params in clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in classifier.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval fine tune plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if fine_tune_eval == True:    \n",
    "#     # Create a figure and a 2x2 grid of subplots\n",
    "#     fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "#     # Confusion matrix subplot\n",
    "#     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axs[0, 0])\n",
    "#     axs[0, 0].set_title('Confusion Matrix')\n",
    "#     axs[0, 0].set_xlabel('Predicted')\n",
    "#     axs[0, 0].set_ylabel('True')\n",
    "\n",
    "#     # F1 Score subplot\n",
    "#     axs[0, 1].text(0.5, 0.5, f'F1 Score: {f1:.2f}', horizontalalignment='center', verticalalignment='center', fontsize=14)\n",
    "#     axs[0, 1].axis('off')\n",
    "\n",
    "#     # Precision subplot\n",
    "#     axs[1, 0].text(0.5, 0.5, f'Precision: {precision:.2f}', horizontalalignment='center', verticalalignment='center', fontsize=14)\n",
    "#     axs[1, 0].axis('off')\n",
    "\n",
    "#     # Recall subplot\n",
    "#     axs[1, 1].text(0.5, 0.5, f'Recall: {recall:.2f}', horizontalalignment='center', verticalalignment='center', fontsize=14)\n",
    "#     axs[1, 1].axis('off')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.title(f'Confusion Matrix of MES-scores - \\n pretrained for: {num_epochs}e, fine tuned for: {num_fine_epochs}e, masking: {mask_ratio} \\n model has {total_params:,.0f} num params. Model used: best (val loss), converged on e-{best_epoch_was}')\n",
    "\n",
    "#     plt.savefig(f'./plots/eval_plot-{eval_plot_name}.png')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fine_tune_eval == True:    \n",
    "    # Create a figure and a 2x1 grid of subplots\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "    # Confusion matrix subplot\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axs[0])\n",
    "    axs[0].set_title('Confusion Matrix')\n",
    "    axs[0].set_xlabel('Predicted')\n",
    "    axs[0].set_ylabel('True')\n",
    "\n",
    "    # F1 Score, Precision, Recall, and Accuracy subplot\n",
    "    axs[1].set_title(f'Pretrained for: {num_epochs}e, fine tuned for: {num_fine_epochs}e, masking ratio: {mask_ratio} \\n model has {total_params:,.0f} num params. \\n Model used: best (val loss), converged on epoch {best_epoch_was}')\n",
    "    axs[1].text(0.5, 0.8, f'F1 Score: {f1:.2f}', horizontalalignment='center', verticalalignment='center', fontsize=14)\n",
    "    axs[1].text(0.5, 0.4, f'Precision: {precision:.2f}', horizontalalignment='center', verticalalignment='center', fontsize=14)\n",
    "    axs[1].text(0.5, 0.2, f'Recall: {recall:.2f}', horizontalalignment='center', verticalalignment='center', fontsize=14)\n",
    "    axs[1].text(0.5, 0.6, f'Accuracy: {accuracy:.2f}', horizontalalignment='center', verticalalignment='center', fontsize=14)\n",
    "    axs[1].axis('off')\n",
    "    \n",
    "    fig.suptitle(f'MES evaluation', fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./plots/eval_plot-{eval_plot_name}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total runtime of this cell: {total_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
